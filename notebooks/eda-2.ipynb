{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6b2b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winutils.exe: C:\\Users\\Rodrigo\\.spark\\hadoop\\bin\\winutils.exe\n",
      "hadoop.dll: C:\\Users\\Rodrigo\\.spark\\hadoop\\bin\\hadoop.dll\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def find_in_path(filename):\n",
    "    for path in os.environ[\"PATH\"].split(os.pathsep):\n",
    "        full_path = os.path.join(path, filename)\n",
    "        if os.path.isfile(full_path):\n",
    "            return full_path\n",
    "    return None\n",
    "\n",
    "print(\"winutils.exe:\", find_in_path(\"winutils.exe\"))\n",
    "print(\"hadoop.dll:\", find_in_path(\"hadoop.dll\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1128ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "from open_finance_lakehouse.utils.spark_session import get_spark_session\n",
    "\n",
    "get_spark_session.cache_clear()\n",
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647e768b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-04</td>\n",
       "      <td>0.049037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-05</td>\n",
       "      <td>0.049037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-06</td>\n",
       "      <td>0.049037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-07</td>\n",
       "      <td>0.049037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-08</td>\n",
       "      <td>0.049037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        data     valor\n",
       "0 2015-05-04  0.049037\n",
       "1 2015-05-05  0.049037\n",
       "2 2015-05-06  0.049037\n",
       "3 2015-05-07  0.049037\n",
       "4 2015-05-08  0.049037"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Ingestão BACEN API\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from open_finance_lakehouse.utils.spark_session import get_spark_session\n",
    "\n",
    "# --- Parameters ---\n",
    "bacen_series_id = 11  # Example: SELIC\n",
    "bacen_start_date = (datetime.today().replace(year=datetime.today().year - 10)).strftime(\"%d/%m/%Y\")\n",
    "bacen_end_date = datetime.today().strftime(\"%d/%m/%Y\")\n",
    "cvm_year = 2024\n",
    "cvm_month = 4\n",
    "\n",
    "\n",
    "bacen_url = (\n",
    "    f\"https://api.bcb.gov.br/dados/serie/bcdata.sgs.{bacen_series_id}/dados\"\n",
    "    f\"?formato=json&dataInicial={bacen_start_date}&dataFinal={bacen_end_date}\"\n",
    ")\n",
    "\n",
    "response = requests.get(bacen_url)\n",
    "\n",
    "df_bacen = pd.read_json(io.StringIO(response.text))\n",
    "df_bacen.columns = [\"data\", \"valor\"]\n",
    "df_bacen[\"data\"] = pd.to_datetime(df_bacen[\"data\"], format=\"%d/%m/%Y\")\n",
    "df_bacen[\"valor\"] = pd.to_numeric(df_bacen[\"valor\"], errors=\"coerce\")\n",
    "df_bacen.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd6b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVM URL: https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_202404.zip\n",
      "CVM Spark DataFrame Schema:\n",
      "root\n",
      " |-- TP_FUNDO_CLASSE: string (nullable = true)\n",
      " |-- CNPJ_FUNDO_CLASSE: string (nullable = true)\n",
      " |-- ID_SUBCLASSE: string (nullable = true)\n",
      " |-- DT_COMPTC: date (nullable = true)\n",
      " |-- VL_TOTAL: double (nullable = true)\n",
      " |-- VL_QUOTA: double (nullable = true)\n",
      " |-- VL_PATRIM_LIQ: double (nullable = true)\n",
      " |-- CAPTC_DIA: double (nullable = true)\n",
      " |-- RESG_DIA: double (nullable = true)\n",
      " |-- NR_COTST: integer (nullable = true)\n",
      "\n",
      "CVM Spark DataFrame Sample:\n",
      "+---------------+------------------+------------+----------+----------+----------+-------------+---------+--------+--------+\n",
      "|TP_FUNDO_CLASSE| CNPJ_FUNDO_CLASSE|ID_SUBCLASSE| DT_COMPTC|  VL_TOTAL|  VL_QUOTA|VL_PATRIM_LIQ|CAPTC_DIA|RESG_DIA|NR_COTST|\n",
      "+---------------+------------------+------------+----------+----------+----------+-------------+---------+--------+--------+\n",
      "|             FI|00.017.024/0001-53|        NULL|2024-04-01|1110061.31|35.0215439|   1110855.35|      0.0|     0.0|       1|\n",
      "|             FI|00.017.024/0001-53|        NULL|2024-04-02| 1110510.5|35.0310826|   1111157.91|      0.0|     0.0|       1|\n",
      "|             FI|00.017.024/0001-53|        NULL|2024-04-03|1110959.34| 35.042477|   1111519.33|      0.0|     0.0|       1|\n",
      "|             FI|00.017.024/0001-53|        NULL|2024-04-04|1111404.79|35.0536951|   1111875.16|      0.0|     0.0|       1|\n",
      "|             FI|00.017.024/0001-53|        NULL|2024-04-05|1108949.19|35.0648921|   1109386.54|      0.0| 2843.78|       1|\n",
      "+---------------+------------------+------------+----------+----------+----------+-------------+---------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "# --- CVM Fetch (Spark) ---\n",
    "cvm_base_url = \"https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/\"\n",
    "cvm_file_name = f\"inf_diario_fi_{cvm_year}{str(cvm_month).zfill(2)}.zip\"\n",
    "cvm_url = f\"{cvm_base_url}{cvm_file_name}\"\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as tmp:\n",
    "    print(\"CVM URL:\", cvm_url)\n",
    "    content = requests.get(cvm_url).content\n",
    "    tmp.write(content)\n",
    "    tmp_path = tmp.name\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(tmp_path, 'r') as zip_ref:\n",
    "    csv_name = zip_ref.namelist()[0]  # Assume only one CSV in the zip\n",
    "    zip_ref.extract(csv_name, os.path.dirname(tmp_path))\n",
    "    csv_path = os.path.join(os.path.dirname(tmp_path), csv_name)\n",
    "\n",
    "# Ensure Spark reads the file before it is deleted\n",
    "cvm_spark_df = spark.read.csv(\n",
    "    csv_path,\n",
    "    header=True,\n",
    "    sep=\";\",\n",
    "    inferSchema=True,\n",
    "    encoding=\"ISO-8859-1\"\n",
    ")\n",
    "\n",
    "# Wait for Spark to finish reading the file before deleting it\n",
    "cvm_spark_df.cache()\n",
    "cvm_spark_df.count()\n",
    "os.unlink(tmp_path)\n",
    "os.unlink(csv_path)\n",
    "\n",
    "\n",
    "print(\"CVM Spark DataFrame Schema:\")\n",
    "cvm_spark_df.printSchema()\n",
    "print(\"CVM Spark DataFrame Sample:\")\n",
    "cvm_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c39c3879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./airflow\n",
      "✅ Bucket 'lakehouse' já existe.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Configurações do MinIO\n",
    "MINIO_ENDPOINT = \"http://localhost:9000\"\n",
    "ACCESS_KEY = os.getenv(\"MINIO_USER\")\n",
    "SECRET_KEY = os.getenv(\"MINIO_PASSWORD\")\n",
    "print(os.getenv(\"AIRFLOW_PROJ_DIR\"))\n",
    "BUCKET_NAME = \"lakehouse\"\n",
    "\n",
    "# Conectar ao MinIO (S3-compatible)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    ")\n",
    "\n",
    "# Checar e criar bucket\n",
    "try:\n",
    "    s3.head_bucket(Bucket=BUCKET_NAME)\n",
    "    print(f\"✅ Bucket '{BUCKET_NAME}' já existe.\")\n",
    "except ClientError as e:\n",
    "    error_code = int(e.response[\"Error\"][\"Code\"])\n",
    "    if error_code == requests.codes.not_found:\n",
    "        print(f\"🔧 Criando bucket '{BUCKET_NAME}'...\")\n",
    "        s3.create_bucket(Bucket=BUCKET_NAME)\n",
    "        print(\"✅ Bucket criado com sucesso.\")\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e749ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Salvar como Delta (Bronze) no MinIO\n",
    "df_bacen_spark = spark.createDataFrame(df_bacen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4633468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bacen_spark.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/bronze/bacen_selic/\")\n",
    "cvm_spark_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/bronze/cvm_if_di/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b61619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transformações da camada Silver\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver_bacen = (\n",
    "    df_bacen_spark\n",
    "    .withColumn(\"ano\", F.year(\"data\"))\n",
    "    .withColumn(\"mes\", F.month(\"data\"))\n",
    "    .withColumn(\"dia\", F.dayofmonth(\"data\"))\n",
    ")\n",
    "\n",
    "silver_cvm = (\n",
    "    cvm_spark_df.withColumn(\"cap_liquida_dia\", F.col(\"CAPTC_DIA\") - F.col(\"RESG_DIA\"))\n",
    "        .withColumn(\"ano\", F.year(\"DT_COMPTC\"))\n",
    "        .withColumn(\"mes\", F.month(\"DT_COMPTC\"))\n",
    "        .withColumn(\"dia\", F.dayofmonth(\"DT_COMPTC\"))\n",
    "        .filter(F.col(\"VL_QUOTA\") > 0)\n",
    "        .filter(F.col(\"VL_PATRIM_LIQ\") > 0)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "silver_bacen.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/silver/bacen_selic/\")\n",
    "silver_cvm.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/silver/cvm_if_di/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccbce57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------+----------+----------+----------+-------------+---------+--------+--------+---------------+----+---+---+\n",
      "|TP_FUNDO_CLASSE|CNPJ_FUNDO_CLASSE |ID_SUBCLASSE|DT_COMPTC |VL_TOTAL  |VL_QUOTA  |VL_PATRIM_LIQ|CAPTC_DIA|RESG_DIA|NR_COTST|cap_liquida_dia|ano |mes|dia|\n",
      "+---------------+------------------+------------+----------+----------+----------+-------------+---------+--------+--------+---------------+----+---+---+\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-01|1110061.31|35.0215439|1110855.35   |0.0      |0.0     |1       |0.0            |2024|4  |1  |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-02|1110510.5 |35.0310826|1111157.91   |0.0      |0.0     |1       |0.0            |2024|4  |2  |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-03|1110959.34|35.042477 |1111519.33   |0.0      |0.0     |1       |0.0            |2024|4  |3  |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-04|1111404.79|35.0536951|1111875.16   |0.0      |0.0     |1       |0.0            |2024|4  |4  |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-05|1108949.19|35.0648921|1109386.54   |0.0      |2843.78 |1       |-2843.78       |2024|4  |5  |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-08|1109152.3 |35.076121 |1109741.8    |0.0      |0.0     |1       |0.0            |2024|4  |8  |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-09|1109600.98|35.0873963|1110098.53   |0.0      |0.0     |1       |0.0            |2024|4  |9  |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-10|1110049.13|35.0986678|1110455.14   |0.0      |0.0     |1       |0.0            |2024|4  |10 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-11|1110499.47|35.1100326|1110814.7    |0.0      |0.0     |1       |0.0            |2024|4  |11 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-12|1110948.6 |35.1213579|1111173.01   |0.0      |0.0     |1       |0.0            |2024|4  |12 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-15|1111332.43|35.1276493|1111372.06   |0.0      |0.0     |1       |0.0            |2024|4  |15 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-16|1111781.7 |35.1339319|1111570.83   |0.0      |0.0     |1       |0.0            |2024|4  |16 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-17|1112222.65|35.1402285|1111770.04   |0.0      |0.0     |1       |0.0            |2024|4  |17 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-18|1112671.54|35.1464374|1111966.48   |0.0      |0.0     |1       |0.0            |2024|4  |18 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-19|1113122.37|35.1527223|1112165.32   |0.0      |0.0     |1       |0.0            |2024|4  |19 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-22|1112782.01|35.1640608|1112524.05   |0.0      |0.0     |1       |0.0            |2024|4  |22 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-23|1113232.57|35.17535  |1112881.22   |0.0      |0.0     |1       |0.0            |2024|4  |23 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-24|1113675.69|35.1866355|1113238.27   |0.0      |0.0     |1       |0.0            |2024|4  |24 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-25|1114077.3 |35.1976855|1113587.87   |0.0      |0.0     |1       |0.0            |2024|4  |25 |\n",
      "|FI             |00.017.024/0001-53|NULL        |2024-04-26|1114520.24|35.2087781|1113938.82   |0.0      |0.0     |1       |0.0            |2024|4  |26 |\n",
      "+---------------+------------------+------------+----------+----------+----------+-------------+---------+--------+--------+---------------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "silver_cvm.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cd8546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean expectation suite created and saved (no validation run, no validator used).\n"
     ]
    }
   ],
   "source": [
    "import great_expectations as gx\n",
    "from great_expectations.core.expectation_configuration import ExpectationConfiguration\n",
    "\n",
    "context = gx.data_context.DataContext(\"gx\")\n",
    "\n",
    "# (Re-)register the asset with the current DataFrame\n",
    "spark_ds = context.get_datasource(\"spark_datasource\")\n",
    "if \"silver_bacen_selic\" in [asset.name for asset in spark_ds.assets]:\n",
    "    spark_ds.delete_asset(\"silver_bacen_selic\")\n",
    "spark_ds.add_dataframe_asset(name=\"silver_bacen_selic\", dataframe=df_bacen_spark)\n",
    "\n",
    "# Overwrite the expectation suite with only valid expectations (NO validation run)\n",
    "suite_name = \"silver_bacen_selic_suite\"\n",
    "if suite_name in [suite.expectation_suite_name for suite in context.list_expectation_suites()]:\n",
    "    context.delete_expectation_suite(suite_name)\n",
    "suite = context.add_expectation_suite(suite_name)\n",
    "\n",
    "# Add expectations directly to the suite (no validator, no data access)\n",
    "suite.expectations = [\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_to_exist\",\n",
    "        kwargs={\"column\": \"data\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_to_exist\",\n",
    "        kwargs={\"column\": \"valor\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\"column\": \"data\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\"column\": \"valor\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_of_type\",\n",
    "        kwargs={\"column\": \"data\", \"type_\": \"TimestampType\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_of_type\",\n",
    "        kwargs={\"column\": \"valor\", \"type_\": \"DoubleType\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"valor\", \"min_value\": 0}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "        kwargs={\"min_value\": 1}\n",
    "    ),\n",
    "]\n",
    "\n",
    "context.save_expectation_suite(suite)\n",
    "print(\"✅ Clean expectation suite created and saved (no validation run, no validator used).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6ee1a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean expectation suite for silver_cvm created and saved (no validation run, no validator used).\n"
     ]
    }
   ],
   "source": [
    "import great_expectations as gx\n",
    "from great_expectations.core.expectation_configuration import ExpectationConfiguration\n",
    "\n",
    "context = gx.data_context.DataContext(\"gx\")\n",
    "\n",
    "# (Re-)register the asset with the current DataFrame\n",
    "spark_ds = context.get_datasource(\"spark_datasource\")\n",
    "if \"silver_cvm_if_di\" in [asset.name for asset in spark_ds.assets]:\n",
    "    spark_ds.delete_asset(\"silver_cvm_if_di\")\n",
    "spark_ds.add_dataframe_asset(name=\"silver_cvm_if_di\", dataframe=silver_cvm)\n",
    "\n",
    "# Overwrite the expectation suite with only valid expectations (NO validation run)\n",
    "suite_name = \"silver_cvm_if_di_suite\"\n",
    "if suite_name in [suite.expectation_suite_name for suite in context.list_expectation_suites()]:\n",
    "    context.delete_expectation_suite(suite_name)\n",
    "suite = context.add_expectation_suite(suite_name)\n",
    "\n",
    "# Add expectations directly to the suite (no validator, no data access)\n",
    "suite.expectations = [\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_to_exist\",\n",
    "        kwargs={\"column\": \"DT_COMPTC\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_to_exist\",\n",
    "        kwargs={\"column\": \"CNPJ_FUNDO_CLASSE\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_to_exist\",\n",
    "        kwargs={\"column\": \"VL_QUOTA\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_to_exist\",\n",
    "        kwargs={\"column\": \"VL_PATRIM_LIQ\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_to_exist\",\n",
    "        kwargs={\"column\": \"cap_liquida_dia\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\"column\": \"DT_COMPTC\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\"column\": \"CNPJ_FUNDO_CLASSE\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\"column\": \"VL_QUOTA\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\"column\": \"VL_PATRIM_LIQ\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_of_type\",\n",
    "        kwargs={\"column\": \"DT_COMPTC\", \"type_\": \"DateType\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_of_type\",\n",
    "        kwargs={\"column\": \"VL_QUOTA\", \"type_\": \"DoubleType\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_of_type\",\n",
    "        kwargs={\"column\": \"VL_PATRIM_LIQ\", \"type_\": \"DoubleType\"}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"VL_QUOTA\", \"min_value\": 0}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"VL_PATRIM_LIQ\", \"min_value\": 0}\n",
    "    ),\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "        kwargs={\"min_value\": 1}\n",
    "    ),\n",
    "]\n",
    "\n",
    "context.save_expectation_suite(suite)\n",
    "print(\"✅ Clean expectation suite for silver_cvm created and saved (no validation run, no validator used).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b878b463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d728e06121485e951b33fcf611961a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743eb7c0c4204f1e938d0b2d7bb2ca20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation Success (bacen & cvm): True\n"
     ]
    }
   ],
   "source": [
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import SimpleCheckpoint\n",
    "\n",
    "context = gx.data_context.DataContext(\"gx\")\n",
    "\n",
    "# Ensure Spark datasource exists\n",
    "if \"spark_datasource\" not in context.datasources:\n",
    "    context.sources.add_spark(name=\"spark_datasource\")\n",
    "\n",
    "# (Re-)register the assets with the current DataFrames\n",
    "spark_ds = context.get_datasource(\"spark_datasource\")\n",
    "asset_names = [asset.name for asset in spark_ds.assets]\n",
    "if \"silver_bacen_selic\" in asset_names:\n",
    "    spark_ds.delete_asset(\"silver_bacen_selic\")\n",
    "if \"silver_cvm_if_di\" in asset_names:\n",
    "    spark_ds.delete_asset(\"silver_cvm_if_di\")\n",
    "spark_ds.add_dataframe_asset(name=\"silver_bacen_selic\", dataframe=df_bacen_spark)\n",
    "spark_ds.add_dataframe_asset(name=\"silver_cvm_if_di\", dataframe=silver_cvm)\n",
    "\n",
    "# Ensure expectation suites exist\n",
    "suite_names = [suite.expectation_suite_name for suite in context.list_expectation_suites()]\n",
    "if \"silver_bacen_selic_suite\" not in suite_names:\n",
    "    context.add_expectation_suite(\"silver_bacen_selic_suite\")\n",
    "if \"silver_cvm_if_di_suite\" not in suite_names:\n",
    "    context.add_expectation_suite(\"silver_cvm_if_di_suite\")\n",
    "\n",
    "# Build batch requests\n",
    "batch_request_bacen = {\n",
    "    \"datasource_name\": \"spark_datasource\",\n",
    "    \"data_asset_name\": \"silver_bacen_selic\",\n",
    "}\n",
    "batch_request_cvm = {\n",
    "    \"datasource_name\": \"spark_datasource\",\n",
    "    \"data_asset_name\": \"silver_cvm_if_di\",\n",
    "}\n",
    "\n",
    "# Run the checkpoint for both assets\n",
    "checkpoint = SimpleCheckpoint(\n",
    "    name=\"silver_combined_checkpoint\",\n",
    "    data_context=context,\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": batch_request_bacen,\n",
    "            \"expectation_suite_name\": \"silver_bacen_selic_suite\",\n",
    "        },\n",
    "        {\n",
    "            \"batch_request\": batch_request_cvm,\n",
    "            \"expectation_suite_name\": \"silver_cvm_if_di_suite\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = checkpoint.run()\n",
    "print(\"✅ Validation Success (bacen & cvm):\", result[\"success\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6075ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Asset: unknown\n",
      "\n",
      "Asset: unknown\n"
     ]
    }
   ],
   "source": [
    "for run_result in result[\"run_results\"].values():\n",
    "    validation_result = run_result[\"validation_result\"]\n",
    "    for res in validation_result[\"results\"]:\n",
    "        if not res[\"success\"]:\n",
    "            print(f\"❌ Failed: {res['expectation_config']['expectation_type']} on column {res['expectation_config']['kwargs'].get('column')}\")\n",
    "            print(f\"    Details: {res['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "690ddce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+--------+-------------------+\n",
      "|inicio             |fim                |min     |max     |media              |\n",
      "+-------------------+-------------------+--------+--------+-------------------+\n",
      "|2015-05-04 00:00:00|2025-04-30 00:00:00|0.007469|0.052531|0.03535479306220097|\n",
      "+-------------------+-------------------+--------+--------+-------------------+\n",
      "\n",
      "+------------------+-----------+----------+---------------+-------------+--------------------+---------------+------------+--------------------+--------------------+\n",
      "|CNPJ_FUNDO_CLASSE |data_inicio|data_fim  |vl_quota_inicio|vl_quota_fim |cap_liquida_total   |cotistas_inicio|cotistas_fim|rentabilidade_pct   |crescimento_cotistas|\n",
      "+------------------+-----------+----------+---------------+-------------+--------------------+---------------+------------+--------------------+--------------------+\n",
      "|00.222.725/0001-24|2024-04-01 |2024-04-30|4898.6985647   |4933.7912316 |-894263.5           |941            |933         |0.7163671419359754  |-8                  |\n",
      "|00.222.816/0001-60|2024-04-01 |2024-04-30|10501.450476   |10582.196823 |-4.397885121E7      |1718           |1659        |0.7689066113727606  |-59                 |\n",
      "|00.306.278/0001-91|2024-04-01 |2024-04-30|26.0665874     |26.2883382   |-2.811091105E7      |5196           |5123        |0.8507089808004511  |-73                 |\n",
      "|00.322.699/0001-06|2024-04-01 |2024-04-30|18.7057185     |18.8781766   |7.961015910999998E7 |2779           |2778        |0.9219538934043087  |-1                  |\n",
      "|00.400.490/0001-13|2024-04-01 |2024-04-30|54338.4175744  |53882.2571015|-1945169.75         |1025           |972         |-0.839480598188987  |-53                 |\n",
      "|00.743.026/0001-20|2024-04-01 |2024-04-30|25.46045       |25.631       |340342.2200000002   |1469           |1456        |0.6698624729727818  |-13                 |\n",
      "|00.822.954/0001-80|2024-04-01 |2024-04-30|15.48845       |15.941639    |-9.579937628999999E7|165            |161         |2.925980327276132   |-4                  |\n",
      "|00.829.288/0001-01|2024-04-01 |2024-04-30|433.557424     |437.310797   |510860.0599999912   |1990           |1976        |0.8657153106435916  |-14                 |\n",
      "|00.836.258/0001-22|2024-04-01 |2024-04-30|61.784235      |61.714145    |-250000.0           |3              |3           |-0.11344317850662136|0                   |\n",
      "|00.840.011/0001-80|2024-04-01 |2024-04-30|43.3489524     |43.7217955   |6.437376264399999E8 |19340          |19470       |0.8600971404328497  |130                 |\n",
      "|00.846.898/0001-13|2024-04-01 |2024-04-30|20.8661351     |20.8485736   |0.0                 |3              |3           |-0.08416268712838604|0                   |\n",
      "|00.947.958/0001-94|2024-04-01 |2024-04-30|72.37429252    |72.6662384   |-2236684.6500000004 |862            |818         |0.40338339738426243 |-44                 |\n",
      "|00.970.059/0001-02|2024-04-01 |2024-04-30|38.4428108     |38.7460465   |-239492.46000000008 |30             |30          |0.7887969003556885  |0                   |\n",
      "|01.103.103/0001-40|2024-04-01 |2024-04-30|13.1152518     |13.0543154   |-100285.6           |3              |3           |-0.4646224176953982 |0                   |\n",
      "|01.221.890/0001-24|2024-04-01 |2024-04-30|249.8523951    |241.028211   |-5502554.6899999995 |673            |668         |-3.531758859653212  |-5                  |\n",
      "|01.340.605/0001-94|2024-04-01 |2024-04-30|3071.863671    |3095.887921  |0.0                 |5              |5           |0.7820740948500229  |0                   |\n",
      "|01.353.626/0001-44|2024-04-01 |2024-04-30|63.10988403    |62.3295868   |-2593060.68         |718            |683         |-1.236410495745926  |-35                 |\n",
      "|01.380.701/0001-66|2024-04-01 |2024-04-30|328.2811611    |329.6070851  |-421128.74          |15             |15          |0.40389890042946675 |0                   |\n",
      "|01.410.032/0001-28|2024-04-01 |2024-04-30|39.287227      |39.615358    |-1.85274234023E9    |1              |1           |0.835210385299016   |0                   |\n",
      "|01.430.938/0001-04|2024-04-01 |2024-04-30|9.8460711      |10.1289475   |-2572469.8400000003 |1298           |1276        |2.8729875818183053  |-22                 |\n",
      "+------------------+-----------+----------+---------------+-------------+--------------------+---------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def run_gold_pipeline():\n",
    "\n",
    "    # Load silver datasets\n",
    "    # silver_cvm = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/cvm/\")\n",
    "    silver_bacen = spark.read.format(\"delta\").load(\"s3a://lakehouse/silver/bacen_selic/\")\n",
    "\n",
    "    # KPIs CVM Fundos\n",
    "    gold_cvm = (\n",
    "        silver_cvm.groupBy(\"CNPJ_FUNDO_CLASSE\")\n",
    "        .agg(\n",
    "            F.min(\"DT_COMPTC\").alias(\"data_inicio\"),\n",
    "            F.max(\"DT_COMPTC\").alias(\"data_fim\"),\n",
    "            F.first(\"VL_QUOTA\", ignorenulls=True).alias(\"vl_quota_inicio\"),\n",
    "            F.last(\"VL_QUOTA\", ignorenulls=True).alias(\"vl_quota_fim\"),\n",
    "            F.sum(\"cap_liquida_dia\").alias(\"cap_liquida_total\"),\n",
    "            F.first(\"NR_COTST\", ignorenulls=True).alias(\"cotistas_inicio\"),\n",
    "            F.last(\"NR_COTST\", ignorenulls=True).alias(\"cotistas_fim\")\n",
    "        )\n",
    "        .withColumn(\"rentabilidade_pct\", ((F.col(\"vl_quota_fim\") - F.col(\"vl_quota_inicio\")) / F.col(\"vl_quota_inicio\")) * 100)\n",
    "        .withColumn(\"crescimento_cotistas\", F.col(\"cotistas_fim\") - F.col(\"cotistas_inicio\"))\n",
    "    )\n",
    "\n",
    "    # KPIs BACEN indicadores\n",
    "    gold_bacen = (\n",
    "        silver_bacen.agg(\n",
    "            F.min(\"data\").alias(\"inicio\"),\n",
    "            F.max(\"data\").alias(\"fim\"),\n",
    "            F.min(\"valor\").alias(\"min\"),\n",
    "            F.max(\"valor\").alias(\"max\"),\n",
    "            F.mean(\"valor\").alias(\"media\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save gold outputs\n",
    "    # gold_cvm.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/gold/cvm_kpis/\")\n",
    "    gold_bacen.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/gold/bacen_kpis/\")\n",
    "    gold_cvm.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://lakehouse/gold/cvm_kpis/\")\n",
    "    return gold_bacen, gold_cvm\n",
    "gold_bacen, gold_cvm = run_gold_pipeline()\n",
    "gold_bacen.show(20,truncate=False)\n",
    "gold_cvm.show(20,truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
