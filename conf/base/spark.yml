# Spark configuration optimized for Windows + MinIO + Delta Lake + Python Worker Connectivity

# Memory and performance - Optimized for small datasets
spark.driver.maxResultSize: 1g
spark.executor.memory: 2g
spark.driver.memory: 2g
spark.sql.shuffle.partitions: 1
spark.serializer: org.apache.spark.serializer.KryoSerializer
spark.default.parallelism: 1

# Adaptive query execution - Optimized for small datasets with ERROR logging
spark.sql.adaptive.enabled: true
spark.sql.adaptive.coalescePartitions.enabled: true
spark.sql.adaptive.coalescePartitions.minPartitionSize: 1MB
spark.sql.adaptive.coalescePartitions.initialPartitionNum: 1
spark.sql.adaptive.logLevel: ERROR

# Python path configuration - critical for Windows (uv-managed Python)
spark.pyspark.driver.python: ${env:PYSPARK_DRIVER_PYTHON}
spark.pyspark.python: ${env:PYSPARK_PYTHON}
spark.executorEnv.PYSPARK_PYTHON: ${env:PYSPARK_PYTHON}
spark.yarn.appMasterEnv.PYSPARK_PYTHON: ${env:PYSPARK_PYTHON}

# Python worker connectivity fixes for Windows
spark.python.worker.reuse: false
spark.python.worker.memory: 1g
spark.task.cpus: 1
spark.executor.cores: 1
spark.python.daemon.module: pyspark.daemon
spark.python.use.daemon: false

# Arrow optimization - Disabled for Windows compatibility
spark.sql.execution.arrow.pyspark.enabled: false

# Logging and debugging - Reduce verbose output and suppress warnings
spark.sql.debug.maxToStringFields: 100
spark.eventLog.enabled: false
spark.ui.showConsoleProgress: false

# Suppress Spark startup verbosity and Ivy dependency resolution
spark.launcher.verbose: false
spark.launcher.quiet: true
spark.python.daemon.log: false
spark.submit.quiet: true

# Additional comprehensive logging suppression
spark.python.worker.log: false

# Additional Java options to suppress logging and Ivy dependency resolution
spark.driver.extraJavaOptions: -Dlog4j.configuration=file:E:/Projetos/Portifolio/Open-Finance-LakeHouse/conf/log4j.properties -Dlog4j.rootLogger=ERROR,console -Dspark.launcher.verbose=false -Dorg.apache.ivy.silence=true -Divy.quiet=true -Divy.message.logger.level=0 -Divy.status.enabled=false -Divy.summary.enabled=false -Divy.report.enabled=false -Divy.console.log.level=0 -Divy.log.modules.in.use=false -Divy.log.resolved.modules=false -Divy.log.downloaded.artifacts=false -Divy.log.resolve.report=false -Divy.log.retrieved.report=false -Divy.log.circular.dependency=false -Divy.log.evicted.modules=false -Divy.log.not.converted.module=false -Divy.log.conflict.resolution=false -Divy.log.resolved.revision=false -Divy.log.default.configuration=false -Divy.log.module.when.found=false -Divy.verbose.level=0 -Divy.log.level=0
spark.executor.extraJavaOptions: -Dlog4j.configuration=file:E:/Projetos/Portifolio/Open-Finance-LakeHouse/conf/log4j.properties -Dlog4j.rootLogger=ERROR,console

# Pre-download and cache dependencies - alternative approach
spark.jars.ivy: E:/Projetos/Portifolio/Open-Finance-LakeHouse/.ivy2

# Windows temp directory cleanup fixes - Suppress warnings and improve cleanup
spark.cleaner.periodicGC.interval: 30s
spark.cleaner.referenceTracking.cleanCheckpoints: true
spark.sql.execution.arrow.maxRecordsPerBatch: 1000

# Windows-specific cleanup improvements
spark.sql.adaptive.localShuffleReader.enabled: false
spark.sql.adaptive.skewJoin.enabled: false
spark.serializer.objectStreamReset: 100

# Session management - Prevent multiple contexts
spark.driver.allowMultipleContexts: false
spark.sql.execution.arrow.pyspark.fallback.enabled: true

# Delta Lake configuration - Using dynamic package resolution
spark.jars.packages: io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4
spark.sql.extensions: io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog

# MinIO S3A configuration with HTTP (no SSL)
spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint: ${env:MINIO_ENDPOINT}
spark.hadoop.fs.s3a.access.key: ${env:MINIO_USER}
spark.hadoop.fs.s3a.secret.key: ${env:MINIO_PASSWORD}
spark.hadoop.fs.s3a.path.style.access: true

# Alternative credential configuration for direct access
spark.hadoop.fs.s3a.endpoint.vanir-proxmox.duckdns.org.access.key: Rodrigo
spark.hadoop.fs.s3a.endpoint.vanir-proxmox.duckdns.org.secret.key: Lazarus17
spark.hadoop.fs.s3a.connection.ssl.enabled: false
spark.hadoop.fs.s3a.endpoint.region: us-east-1
spark.hadoop.fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# S3A Performance and reliability settings for Windows - Optimized for small files
spark.hadoop.fs.s3a.connection.maximum: 10
spark.hadoop.fs.s3a.connection.timeout: 10000
spark.hadoop.fs.s3a.socket.recv.buffer: 65536
spark.hadoop.fs.s3a.socket.send.buffer: 65536
spark.hadoop.fs.s3a.threads.max: 5
spark.hadoop.fs.s3a.max.total.tasks: 5
spark.hadoop.fs.s3a.attempts.maximum: 3
spark.hadoop.fs.s3a.socket.timeout: 10000

# Retry and timeout settings
spark.hadoop.fs.s3a.retry.limit: 3
spark.hadoop.fs.s3a.retry.interval: 1s
spark.hadoop.fs.s3a.connection.establish.timeout: 30000

# Multipart upload settings - Optimized for small files
spark.hadoop.fs.s3a.multipart.size: 5242880
spark.hadoop.fs.s3a.multipart.threshold: 10485760
spark.hadoop.fs.s3a.fast.upload: true
spark.hadoop.fs.s3a.fast.upload.buffer: array

# Disable problematic features for MinIO
spark.hadoop.fs.s3a.server-side-encryption-algorithm: ""
spark.hadoop.fs.s3a.change.detection.mode: none
spark.hadoop.fs.s3a.change.detection.version.required: false

# Hadoop metrics configuration - Disable all metrics to eliminate warnings
spark.hadoop.hadoop.metrics2.config.file: E:/Projetos/Portifolio/Open-Finance-LakeHouse/conf/hadoop-metrics2.properties
spark.hadoop.fs.s3a.metrics.enabled: false
spark.hadoop.hadoop.metrics2.s3a-file-system.source.class: org.apache.hadoop.metrics2.source.NullMetricsSource

# Scheduling
spark.scheduler.mode: FAIR