{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db43b459",
   "metadata": {},
   "source": [
    "# 🏦 BACEN Economic Data Visualization\n",
    "\n",
    "**Brazilian Central Bank (BACEN) Financial Time Series Analysis**\n",
    "\n",
    "This notebook provides comprehensive visualization and analysis of Brazilian economic indicators from BACEN (Banco Central do Brasil), including:\n",
    "\n",
    "- 📈 **Interest Rates**: SELIC rate, CDI, over rate, SELIC target\n",
    "- \udcb0 **Exchange Rates**: USD/BRL, EUR/BRL \n",
    "- \udcca **Inflation Indices**: IPCA, INPC, IGP-M, IGP-DI, IGP-10\n",
    "- 🏛️ **Economic Indicators**: Government debt/GDP ratio, international reserves, GDP forecasts\n",
    "- 📋 **Financial Instruments**: TLP (Long-term Rate)\n",
    "\n",
    "**Data Sources**: \n",
    "- Local raw data: 4 BACEN series\n",
    "- MinIO data lake: 13 additional BACEN series\n",
    "- **Total**: 17 economic time series with historical data from 1944 to 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c64ae",
   "metadata": {},
   "source": [
    "## 🔧 Environment Setup\n",
    "\n",
    "Initialize the Python environment with all necessary libraries and establish connections to both local data files and the MinIO data lake infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b631de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌟 LAKEHOUSE DATA VISUALIZATION ENVIRONMENT SETUP\n",
    "# Este código configura o ambiente Python necessário para análise de dados do lakehouse brasileiro\n",
    "\n",
    "# Importação de bibliotecas essenciais\n",
    "import os  # Para acessar variáveis de ambiente do sistema operacional\n",
    "import pandas as pd  # Para manipulação e análise de dados estruturados\n",
    "import io  # Para operações de entrada/saída, especialmente com streams de bytes\n",
    "import warnings  # Para controlar exibição de avisos/warnings\n",
    "warnings.filterwarnings('ignore')  # Suprime warnings para saída mais limpa\n",
    "\n",
    "# Carregamento de variáveis de ambiente de arquivo .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Carrega configurações do arquivo .env para as variáveis de ambiente\n",
    "\n",
    "# Configuração do cliente MinIO para acesso ao data lake\n",
    "from minio import Minio  # Cliente Python para MinIO (storage S3-compatível)\n",
    "\n",
    "# Dicionário de configuração do MinIO usando variáveis de ambiente com fallbacks\n",
    "MINIO_CONFIG = {\n",
    "    \"endpoint\": os.getenv(\"MINIO_ENDPOINT\", \"localhost:9000\"),  # Endereço do servidor MinIO\n",
    "    \"access_key\": os.getenv(\"MINIO_USER\", \"minioadmin\"),        # Chave de acesso (usuário)\n",
    "    \"secret_key\": os.getenv(\"MINIO_PASSWORD\", \"minioadmin\"),    # Chave secreta (senha)\n",
    "    \"bucket_name\": os.getenv(\"MINIO_BUCKET\", \"lakehouse\")       # Nome do bucket onde estão os dados\n",
    "}\n",
    "\n",
    "# Sanitização do endpoint para garantir formato correto\n",
    "import re\n",
    "endpoint = MINIO_CONFIG[\"endpoint\"]\n",
    "# Remove protocolo (http:// ou https://) se presente\n",
    "endpoint = re.sub(r\"^https?://\", \"\", endpoint)  \n",
    "# Remove qualquer caminho após o domínio/IP\n",
    "endpoint = endpoint.split(\"/\")[0]  \n",
    "\n",
    "# Inicialização do cliente MinIO com configurações sanitizadas\n",
    "minio_client = Minio(\n",
    "    endpoint,  # Endpoint limpo (apenas host:porta)\n",
    "    access_key=MINIO_CONFIG[\"access_key\"],  # Credenciais de acesso\n",
    "    secret_key=MINIO_CONFIG[\"secret_key\"],  # Credenciais secretas\n",
    "    secure=MINIO_CONFIG[\"endpoint\"].startswith(\"https\")  # SSL se endpoint usar HTTPS\n",
    ")\n",
    "\n",
    "print(\"✅ Ambiente configurado com sucesso!\")\n",
    "print(f\"🔗 MinIO Endpoint: {endpoint}\")\n",
    "print(f\"📦 Bucket: {MINIO_CONFIG['bucket_name']}\")\n",
    "print(\"🚀 Pronto para descoberta e análise de dados do lakehouse!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc5fb2",
   "metadata": {},
   "source": [
    "# 🏦 Brazilian Financial Market Data Visualization\n",
    "\n",
    "This notebook provides comprehensive visualization and analysis of Brazilian financial and economic data from multiple sources:\n",
    "\n",
    "## 📊 **Data Sources:**\n",
    "\n",
    "### 🏛️ **BACEN (Central Bank) Economic Indicators:**\n",
    "- SELIC rate, CDI, exchange rates (USD/BRL, EUR/BRL)\n",
    "- Inflation indices (IPCA, INPC, IGP-M, IGP-DI, IGP-10)\n",
    "- Government debt/GDP ratio, international reserves, GDP forecasts\n",
    "\n",
    "### 📈 **B3 (Stock Exchange) Market Data:**\n",
    "- Stock market indices and financial instruments\n",
    "- Trading volumes and market indicators\n",
    "\n",
    "### 🌍 **Yahoo Finance International Data:**\n",
    "- Brazilian ETFs (BOVA11, SMAL11, SPXI11, etc.)\n",
    "- Commodities (Oil, Coffee, Soybeans, Gold)\n",
    "- Currency pairs and international indices\n",
    "\n",
    "### 📋 **IBGE & IPEA Economic Statistics:**\n",
    "- Consumer price indices\n",
    "- Government revenue and fiscal data\n",
    "\n",
    "Let's start by exploring what data is available across all these sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING COMPREHENSIVE PARQUET/DELTA FORMAT DATA DISCOVERY...\n",
      "💰 DISCOVERING ALL PARQUET/DELTA FORMAT DATA SOURCES\n",
      "=================================================================\n",
      "🔍 Reading from data lake layers...\n",
      "🏛️ COMPREHENSIVE BACEN DATA DISCOVERY:\n",
      "=============================================\n",
      "\n",
      "🥉 READING BACEN BRONZE LAYER DATA:\n",
      "----------------------------------------\n",
      "📁 Found 18 BACEN bronze layer files\n",
      "📈 Reading bronze/bacen_cdi/part-00000-f707704a-3bce-4ca5-b133-6bc7e1fbae72-c000.snappy.parquet...\n",
      "📁 Found 18 BACEN bronze layer files\n",
      "📈 Reading bronze/bacen_cdi/part-00000-f707704a-3bce-4ca5-b133-6bc7e1fbae72-c000.snappy.parquet...\n",
      "   ✅ Bacen Cdi/Part-00000-F707704A-3Bce-4Ca5-B133-6Bc7E1Fbae72-C000.Snappy: 9,848 records\n",
      "📈 Reading bronze/bacen_eur_brl/part-00000-89297f99-39ac-4233-b1dd-ea2551bd04ab-c000.snappy.parquet...\n",
      "   ✅ Bacen Eur Brl/Part-00000-89297F99-39Ac-4233-B1Dd-Ea2551Bd04Ab-C000.Snappy: 6,662 records\n",
      "📈 Reading bronze/bacen_igp_10/part-00000-69f59842-ba26-4197-bd10-11ed23d5e659-c000.snappy.parquet...\n",
      "   ✅ Bacen Cdi/Part-00000-F707704A-3Bce-4Ca5-B133-6Bc7E1Fbae72-C000.Snappy: 9,848 records\n",
      "📈 Reading bronze/bacen_eur_brl/part-00000-89297f99-39ac-4233-b1dd-ea2551bd04ab-c000.snappy.parquet...\n",
      "   ✅ Bacen Eur Brl/Part-00000-89297F99-39Ac-4233-B1Dd-Ea2551Bd04Ab-C000.Snappy: 6,662 records\n",
      "📈 Reading bronze/bacen_igp_10/part-00000-69f59842-ba26-4197-bd10-11ed23d5e659-c000.snappy.parquet...\n",
      "   ✅ Bacen Igp 10/Part-00000-69F59842-Ba26-4197-Bd10-11Ed23D5E659-C000.Snappy: 373 records\n",
      "📈 Reading bronze/bacen_igp_di/part-00000-88bca5ea-3187-4930-b7df-1394799d29d9-c000.snappy.parquet...\n",
      "   ✅ Bacen Igp Di/Part-00000-88Bca5Ea-3187-4930-B7Df-1394799D29D9-C000.Snappy: 436 records\n",
      "📈 Reading bronze/bacen_igp_m/part-00000-f635231b-b320-4801-b7d1-50a2c46d1370-c000.snappy.parquet...\n",
      "   ✅ Bacen Igp M/Part-00000-F635231B-B320-4801-B7D1-50A2C46D1370-C000.Snappy: 985 records\n",
      "📈 Reading bronze/bacen_inpc/part-00000-df681ed7-552b-4a40-bd06-3b5d67a1605e-c000.snappy.parquet...\n",
      "   ✅ Bacen Igp 10/Part-00000-69F59842-Ba26-4197-Bd10-11Ed23D5E659-C000.Snappy: 373 records\n",
      "📈 Reading bronze/bacen_igp_di/part-00000-88bca5ea-3187-4930-b7df-1394799d29d9-c000.snappy.parquet...\n",
      "   ✅ Bacen Igp Di/Part-00000-88Bca5Ea-3187-4930-B7Df-1394799D29D9-C000.Snappy: 436 records\n",
      "📈 Reading bronze/bacen_igp_m/part-00000-f635231b-b320-4801-b7d1-50a2c46d1370-c000.snappy.parquet...\n",
      "   ✅ Bacen Igp M/Part-00000-F635231B-B320-4801-B7D1-50A2C46D1370-C000.Snappy: 985 records\n",
      "📈 Reading bronze/bacen_inpc/part-00000-df681ed7-552b-4a40-bd06-3b5d67a1605e-c000.snappy.parquet...\n",
      "   ✅ Bacen Inpc/Part-00000-Df681Ed7-552B-4A40-Bd06-3B5D67A1605E-C000.Snappy: 559 records\n",
      "📈 Reading bronze/bacen_ipca/part-00000-4f565512-b3a8-443c-b79f-4fdd2429e510-c000.snappy.parquet...\n",
      "   ✅ Bacen Ipca/Part-00000-4F565512-B3A8-443C-B79F-4Fdd2429E510-C000.Snappy: 550 records\n",
      "📈 Reading bronze/bacen_ipca/part-00000-fe20e3f6-c178-4bc6-8066-0d92e30928d6-c000.snappy.parquet...\n",
      "   ✅ Bacen Ipca/Part-00000-Fe20E3F6-C178-4Bc6-8066-0D92E30928D6-C000.Snappy: 550 records\n",
      "📈 Reading bronze/bacen_ipca_15/part-00000-43fd73c2-4d03-462f-99de-06a1908dbba5-c000.snappy.parquet...\n",
      "   ✅ Bacen Inpc/Part-00000-Df681Ed7-552B-4A40-Bd06-3B5D67A1605E-C000.Snappy: 559 records\n",
      "📈 Reading bronze/bacen_ipca/part-00000-4f565512-b3a8-443c-b79f-4fdd2429e510-c000.snappy.parquet...\n",
      "   ✅ Bacen Ipca/Part-00000-4F565512-B3A8-443C-B79F-4Fdd2429E510-C000.Snappy: 550 records\n",
      "📈 Reading bronze/bacen_ipca/part-00000-fe20e3f6-c178-4bc6-8066-0d92e30928d6-c000.snappy.parquet...\n",
      "   ✅ Bacen Ipca/Part-00000-Fe20E3F6-C178-4Bc6-8066-0D92E30928D6-C000.Snappy: 550 records\n",
      "📈 Reading bronze/bacen_ipca_15/part-00000-43fd73c2-4d03-462f-99de-06a1908dbba5-c000.snappy.parquet...\n",
      "   ✅ Bacen Ipca 15/Part-00000-43Fd73C2-4D03-462F-99De-06A1908Dbba5-C000.Snappy: 303 records\n",
      "📈 Reading bronze/bacen_over/part-00000-a848f716-2ebf-49e2-9daf-89a92a99e968-c000.snappy.parquet...\n",
      "   ✅ Bacen Over/Part-00000-A848F716-2Ebf-49E2-9Daf-89A92A99E968-C000.Snappy: 9,807 records\n",
      "📈 Reading bronze/bacen_selic/part-00000-953105a6-fa85-4b2e-90e8-31e2f628c50d-c000.snappy.parquet...\n",
      "   ✅ Bacen Ipca 15/Part-00000-43Fd73C2-4D03-462F-99De-06A1908Dbba5-C000.Snappy: 303 records\n",
      "📈 Reading bronze/bacen_over/part-00000-a848f716-2ebf-49e2-9daf-89a92a99e968-c000.snappy.parquet...\n",
      "   ✅ Bacen Over/Part-00000-A848F716-2Ebf-49E2-9Daf-89A92A99E968-C000.Snappy: 9,807 records\n",
      "📈 Reading bronze/bacen_selic/part-00000-953105a6-fa85-4b2e-90e8-31e2f628c50d-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic/Part-00000-953105A6-Fa85-4B2E-90E8-31E2F628C50D-C000.Snappy: 9,806 records\n",
      "📈 Reading bronze/bacen_selic/part-00000-af830f2f-cbf2-4822-bfe0-4217477d64d3-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic/Part-00000-Af830F2F-Cbf2-4822-Bfe0-4217477D64D3-C000.Snappy: 9,806 records\n",
      "📈 Reading bronze/bacen_selic/part-00000-df9b8d4f-4086-4a2a-883f-3430602d6370-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic/Part-00000-953105A6-Fa85-4B2E-90E8-31E2F628C50D-C000.Snappy: 9,806 records\n",
      "📈 Reading bronze/bacen_selic/part-00000-af830f2f-cbf2-4822-bfe0-4217477d64d3-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic/Part-00000-Af830F2F-Cbf2-4822-Bfe0-4217477D64D3-C000.Snappy: 9,806 records\n",
      "📈 Reading bronze/bacen_selic/part-00000-df9b8d4f-4086-4a2a-883f-3430602d6370-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic/Part-00000-Df9B8D4F-4086-4A2A-883F-3430602D6370-C000.Snappy: 9,807 records\n",
      "📈 Reading bronze/bacen_selic/part-00000-e70619e3-c38d-49cf-8b30-044ad19485f1-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic/Part-00000-E70619E3-C38D-49Cf-8B30-044Ad19485F1-C000.Snappy: 9,806 records\n",
      "📈 Reading bronze/bacen_selic_meta/part-00000-087fbc5c-82af-48ad-babc-a614bbb8aeb2-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic/Part-00000-Df9B8D4F-4086-4A2A-883F-3430602D6370-C000.Snappy: 9,807 records\n",
      "📈 Reading bronze/bacen_selic/part-00000-e70619e3-c38d-49cf-8b30-044ad19485f1-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic/Part-00000-E70619E3-C38D-49Cf-8B30-044Ad19485F1-C000.Snappy: 9,806 records\n",
      "📈 Reading bronze/bacen_selic_meta/part-00000-087fbc5c-82af-48ad-babc-a614bbb8aeb2-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic Meta/Part-00000-087Fbc5C-82Af-48Ad-Babc-A614Bbb8Aeb2-C000.Snappy: 9,633 records\n",
      "📈 Reading bronze/bacen_tlp/part-00000-b9c9f40e-b7a1-4198-8279-c7b053ed8448-c000.snappy.parquet...\n",
      "   ✅ Bacen Tlp/Part-00000-B9C9F40E-B7A1-4198-8279-C7B053Ed8448-C000.Snappy: 46 records\n",
      "📈 Reading bronze/bacen_usd_brl/part-00000-e3777257-fcb3-4f2c-82f1-3ed7a803655c-c000.snappy.parquet...\n",
      "   ✅ Bacen Selic Meta/Part-00000-087Fbc5C-82Af-48Ad-Babc-A614Bbb8Aeb2-C000.Snappy: 9,633 records\n",
      "📈 Reading bronze/bacen_tlp/part-00000-b9c9f40e-b7a1-4198-8279-c7b053ed8448-c000.snappy.parquet...\n",
      "   ✅ Bacen Tlp/Part-00000-B9C9F40E-B7A1-4198-8279-C7B053Ed8448-C000.Snappy: 46 records\n",
      "📈 Reading bronze/bacen_usd_brl/part-00000-e3777257-fcb3-4f2c-82f1-3ed7a803655c-c000.snappy.parquet...\n",
      "   ✅ Bacen Usd Brl/Part-00000-E3777257-Fcb3-4F2C-82F1-3Ed7A803655C-C000.Snappy: 10,182 records\n",
      "📈 Reading bronze/bacen_usd_brl/part-00000-fd14e1a9-98ca-4627-b93d-1a76b370edc4-c000.snappy.parquet...\n",
      "   ✅ Bacen Usd Brl/Part-00000-E3777257-Fcb3-4F2C-82F1-3Ed7A803655C-C000.Snappy: 10,182 records\n",
      "📈 Reading bronze/bacen_usd_brl/part-00000-fd14e1a9-98ca-4627-b93d-1a76b370edc4-c000.snappy.parquet...\n",
      "   ✅ Bacen Usd Brl/Part-00000-Fd14E1A9-98Ca-4627-B93D-1A76B370Edc4-C000.Snappy: 10,182 records\n",
      "\n",
      "📊 BACEN DATA SUMMARY:\n",
      "-------------------------\n",
      "✅ BACEN_BRONZE_Bacen Cdi/Part-00000-F707704A-3Bce-4Ca5-B133-6Bc7E1Fbae72-C000.Snappy: 9,848 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_cdi/part-00000-f707704a-3bce-4ca5-b133-6bc7e1fbae72-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Eur Brl/Part-00000-89297F99-39Ac-4233-B1Dd-Ea2551Bd04Ab-C000.Snappy: 6,662 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_eur_brl/part-00000-89297f99-39ac-4233-b1dd-ea2551bd04ab-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Igp 10/Part-00000-69F59842-Ba26-4197-Bd10-11Ed23D5E659-C000.Snappy: 373 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_igp_10/part-00000-69f59842-ba26-4197-bd10-11ed23d5e659-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Igp Di/Part-00000-88Bca5Ea-3187-4930-B7Df-1394799D29D9-C000.Snappy: 436 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_igp_di/part-00000-88bca5ea-3187-4930-b7df-1394799d29d9-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Igp M/Part-00000-F635231B-B320-4801-B7D1-50A2C46D1370-C000.Snappy: 985 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_igp_m/part-00000-f635231b-b320-4801-b7d1-50a2c46d1370-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Inpc/Part-00000-Df681Ed7-552B-4A40-Bd06-3B5D67A1605E-C000.Snappy: 559 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_inpc/part-00000-df681ed7-552b-4a40-bd06-3b5d67a1605e-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Ipca/Part-00000-4F565512-B3A8-443C-B79F-4Fdd2429E510-C000.Snappy: 550 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_ipca/part-00000-4f565512-b3a8-443c-b79f-4fdd2429e510-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Ipca/Part-00000-Fe20E3F6-C178-4Bc6-8066-0D92E30928D6-C000.Snappy: 550 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_ipca/part-00000-fe20e3f6-c178-4bc6-8066-0d92e30928d6-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Ipca 15/Part-00000-43Fd73C2-4D03-462F-99De-06A1908Dbba5-C000.Snappy: 303 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_ipca_15/part-00000-43fd73c2-4d03-462f-99de-06a1908dbba5-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Over/Part-00000-A848F716-2Ebf-49E2-9Daf-89A92A99E968-C000.Snappy: 9,807 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_over/part-00000-a848f716-2ebf-49e2-9daf-89a92a99e968-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic/Part-00000-953105A6-Fa85-4B2E-90E8-31E2F628C50D-C000.Snappy: 9,806 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic/part-00000-953105a6-fa85-4b2e-90e8-31e2f628c50d-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic/Part-00000-Af830F2F-Cbf2-4822-Bfe0-4217477D64D3-C000.Snappy: 9,806 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic/part-00000-af830f2f-cbf2-4822-bfe0-4217477d64d3-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic/Part-00000-Df9B8D4F-4086-4A2A-883F-3430602D6370-C000.Snappy: 9,807 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic/part-00000-df9b8d4f-4086-4a2a-883f-3430602d6370-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic/Part-00000-E70619E3-C38D-49Cf-8B30-044Ad19485F1-C000.Snappy: 9,806 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic/part-00000-e70619e3-c38d-49cf-8b30-044ad19485f1-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic Meta/Part-00000-087Fbc5C-82Af-48Ad-Babc-A614Bbb8Aeb2-C000.Snappy: 9,633 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic_meta/part-00000-087fbc5c-82af-48ad-babc-a614bbb8aeb2-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Tlp/Part-00000-B9C9F40E-B7A1-4198-8279-C7B053Ed8448-C000.Snappy: 46 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_tlp/part-00000-b9c9f40e-b7a1-4198-8279-c7b053ed8448-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Usd Brl/Part-00000-E3777257-Fcb3-4F2C-82F1-3Ed7A803655C-C000.Snappy: 10,182 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_usd_brl/part-00000-e3777257-fcb3-4f2c-82f1-3ed7a803655c-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Usd Brl/Part-00000-Fd14E1A9-98Ca-4627-B93D-1A76B370Edc4-C000.Snappy: 10,182 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_usd_brl/part-00000-fd14e1a9-98ca-4627-b93d-1a76b370edc4-c000.snappy.parquet\n",
      "\n",
      "🥈 READING SILVER LAYER PARQUET DATA:\n",
      "----------------------------------------\n",
      "📁 Found 115 silver layer parquet files\n",
      "📈 Reading IMA_B series (20 files)...\n",
      "   ✅ Bacen Usd Brl/Part-00000-Fd14E1A9-98Ca-4627-B93D-1A76B370Edc4-C000.Snappy: 10,182 records\n",
      "\n",
      "📊 BACEN DATA SUMMARY:\n",
      "-------------------------\n",
      "✅ BACEN_BRONZE_Bacen Cdi/Part-00000-F707704A-3Bce-4Ca5-B133-6Bc7E1Fbae72-C000.Snappy: 9,848 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_cdi/part-00000-f707704a-3bce-4ca5-b133-6bc7e1fbae72-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Eur Brl/Part-00000-89297F99-39Ac-4233-B1Dd-Ea2551Bd04Ab-C000.Snappy: 6,662 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_eur_brl/part-00000-89297f99-39ac-4233-b1dd-ea2551bd04ab-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Igp 10/Part-00000-69F59842-Ba26-4197-Bd10-11Ed23D5E659-C000.Snappy: 373 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_igp_10/part-00000-69f59842-ba26-4197-bd10-11ed23d5e659-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Igp Di/Part-00000-88Bca5Ea-3187-4930-B7Df-1394799D29D9-C000.Snappy: 436 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_igp_di/part-00000-88bca5ea-3187-4930-b7df-1394799d29d9-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Igp M/Part-00000-F635231B-B320-4801-B7D1-50A2C46D1370-C000.Snappy: 985 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_igp_m/part-00000-f635231b-b320-4801-b7d1-50a2c46d1370-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Inpc/Part-00000-Df681Ed7-552B-4A40-Bd06-3B5D67A1605E-C000.Snappy: 559 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_inpc/part-00000-df681ed7-552b-4a40-bd06-3b5d67a1605e-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Ipca/Part-00000-4F565512-B3A8-443C-B79F-4Fdd2429E510-C000.Snappy: 550 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_ipca/part-00000-4f565512-b3a8-443c-b79f-4fdd2429e510-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Ipca/Part-00000-Fe20E3F6-C178-4Bc6-8066-0D92E30928D6-C000.Snappy: 550 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_ipca/part-00000-fe20e3f6-c178-4bc6-8066-0d92e30928d6-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Ipca 15/Part-00000-43Fd73C2-4D03-462F-99De-06A1908Dbba5-C000.Snappy: 303 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_ipca_15/part-00000-43fd73c2-4d03-462f-99de-06a1908dbba5-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Over/Part-00000-A848F716-2Ebf-49E2-9Daf-89A92A99E968-C000.Snappy: 9,807 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_over/part-00000-a848f716-2ebf-49e2-9daf-89a92a99e968-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic/Part-00000-953105A6-Fa85-4B2E-90E8-31E2F628C50D-C000.Snappy: 9,806 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic/part-00000-953105a6-fa85-4b2e-90e8-31e2f628c50d-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic/Part-00000-Af830F2F-Cbf2-4822-Bfe0-4217477D64D3-C000.Snappy: 9,806 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic/part-00000-af830f2f-cbf2-4822-bfe0-4217477d64d3-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic/Part-00000-Df9B8D4F-4086-4A2A-883F-3430602D6370-C000.Snappy: 9,807 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic/part-00000-df9b8d4f-4086-4a2a-883f-3430602d6370-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic/Part-00000-E70619E3-C38D-49Cf-8B30-044Ad19485F1-C000.Snappy: 9,806 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic/part-00000-e70619e3-c38d-49cf-8b30-044ad19485f1-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Selic Meta/Part-00000-087Fbc5C-82Af-48Ad-Babc-A614Bbb8Aeb2-C000.Snappy: 9,633 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_selic_meta/part-00000-087fbc5c-82af-48ad-babc-a614bbb8aeb2-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Tlp/Part-00000-B9C9F40E-B7A1-4198-8279-C7B053Ed8448-C000.Snappy: 46 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_tlp/part-00000-b9c9f40e-b7a1-4198-8279-c7b053ed8448-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Usd Brl/Part-00000-E3777257-Fcb3-4F2C-82F1-3Ed7A803655C-C000.Snappy: 10,182 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_usd_brl/part-00000-e3777257-fcb3-4f2c-82f1-3ed7a803655c-c000.snappy.parquet\n",
      "✅ BACEN_BRONZE_Bacen Usd Brl/Part-00000-Fd14E1A9-98Ca-4627-B93D-1A76B370Edc4-C000.Snappy: 10,182 records (BACEN Bronze)\n",
      "   📁 File: bronze/bacen_usd_brl/part-00000-fd14e1a9-98ca-4627-b93d-1a76b370edc4-c000.snappy.parquet\n",
      "\n",
      "🥈 READING SILVER LAYER PARQUET DATA:\n",
      "----------------------------------------\n",
      "📁 Found 115 silver layer parquet files\n",
      "📈 Reading IMA_B series (20 files)...\n",
      "   ✅ IMA_B: 20 records\n",
      "📈 Reading IMA_B_5 series (20 files)...\n",
      "   ✅ IMA_B: 20 records\n",
      "📈 Reading IMA_B_5 series (20 files)...\n",
      "   ✅ IMA_B_5: 20 records\n",
      "\n",
      "🥇 READING GOLD LAYER PARQUET DATA:\n",
      "--------------------------------------\n",
      "📁 Found 58 gold layer parquet files\n",
      "📈 Reading gold/cdi_kpis/part-00000-8bafb8d7-0943-4f97-8717-b7171049d197-c000.snappy.parquet...\n",
      "   ✅ Cdi Kpis: 473 records\n",
      "📈 Reading gold/divida_pib/part-00000-41f770b4-88e0-47d8-8a7e-a0f06f40956e-c000.snappy.parquet...\n",
      "   ✅ IMA_B_5: 20 records\n",
      "\n",
      "🥇 READING GOLD LAYER PARQUET DATA:\n",
      "--------------------------------------\n",
      "📁 Found 58 gold layer parquet files\n",
      "📈 Reading gold/cdi_kpis/part-00000-8bafb8d7-0943-4f97-8717-b7171049d197-c000.snappy.parquet...\n",
      "   ✅ Cdi Kpis: 473 records\n",
      "📈 Reading gold/divida_pib/part-00000-41f770b4-88e0-47d8-8a7e-a0f06f40956e-c000.snappy.parquet...\n",
      "   ✅ Divida Pib: 282 records\n",
      "📈 Reading gold/divida_pib/part-00000-89beb9c5-8f62-40d8-a196-8bd5af207b83-c000.snappy.parquet...\n",
      "   ✅ Divida Pib: 282 records\n",
      "📈 Reading gold/eur_brl_kpis/part-00000-d726f7d4-1cee-4e9d-8395-453efb4f1bd2-c000.snappy.parquet...\n",
      "   ✅ Eur Brl Kpis: 320 records\n",
      "📈 Reading gold/focus_pib/part-00000-1ab7f4dc-70f4-4dab-86b8-8237751c0770-c000.snappy.parquet...\n",
      "   ✅ Divida Pib: 282 records\n",
      "📈 Reading gold/divida_pib/part-00000-89beb9c5-8f62-40d8-a196-8bd5af207b83-c000.snappy.parquet...\n",
      "   ✅ Divida Pib: 282 records\n",
      "📈 Reading gold/eur_brl_kpis/part-00000-d726f7d4-1cee-4e9d-8395-453efb4f1bd2-c000.snappy.parquet...\n",
      "   ✅ Eur Brl Kpis: 320 records\n",
      "📈 Reading gold/focus_pib/part-00000-1ab7f4dc-70f4-4dab-86b8-8237751c0770-c000.snappy.parquet...\n",
      "   ✅ Focus Pib: 473 records\n",
      "📈 Reading gold/focus_pib/part-00000-92f5589a-89b7-4175-84ce-d386363f434b-c000.snappy.parquet...\n",
      "   ✅ Focus Pib: 473 records\n",
      "📈 Reading gold/igp_10_kpis/part-00000-d195673e-0cc8-4c0c-95a2-11c6cafe07d6-c000.snappy.parquet...\n",
      "   ✅ Igp 10 Kpis: 370 records\n",
      "📈 Reading gold/igp_di_kpis/part-00000-0dc5b527-679d-468d-8a89-98f8275f8b97-c000.snappy.parquet...\n",
      "   ✅ Focus Pib: 473 records\n",
      "📈 Reading gold/focus_pib/part-00000-92f5589a-89b7-4175-84ce-d386363f434b-c000.snappy.parquet...\n",
      "   ✅ Focus Pib: 473 records\n",
      "📈 Reading gold/igp_10_kpis/part-00000-d195673e-0cc8-4c0c-95a2-11c6cafe07d6-c000.snappy.parquet...\n",
      "   ✅ Igp 10 Kpis: 370 records\n",
      "📈 Reading gold/igp_di_kpis/part-00000-0dc5b527-679d-468d-8a89-98f8275f8b97-c000.snappy.parquet...\n",
      "   ✅ Igp Di Kpis: 432 records\n",
      "📈 Reading gold/igp_m_kpis/part-00000-bec4e06b-985d-409e-b75b-bbc78ae358e8-c000.snappy.parquet...\n",
      "   ✅ Igp M Kpis: 976 records\n",
      "📈 Reading gold/inpc_kpis/part-00000-0669c47d-66d6-4a92-934a-50cd03cffddd-c000.snappy.parquet...\n",
      "   ✅ Inpc Kpis: 554 records\n",
      "📈 Reading gold/ipca_15_kpis/part-00000-eaaffd9b-2203-4d6c-8021-4518f449bbbe-c000.snappy.parquet...\n",
      "   ✅ Igp Di Kpis: 432 records\n",
      "📈 Reading gold/igp_m_kpis/part-00000-bec4e06b-985d-409e-b75b-bbc78ae358e8-c000.snappy.parquet...\n",
      "   ✅ Igp M Kpis: 976 records\n",
      "📈 Reading gold/inpc_kpis/part-00000-0669c47d-66d6-4a92-934a-50cd03cffddd-c000.snappy.parquet...\n",
      "   ✅ Inpc Kpis: 554 records\n",
      "📈 Reading gold/ipca_15_kpis/part-00000-eaaffd9b-2203-4d6c-8021-4518f449bbbe-c000.snappy.parquet...\n",
      "   ✅ Ipca 15 Kpis: 301 records\n",
      "📈 Reading gold/ipca_kpis/part-00000-124abc5b-1595-43cb-8e14-6055e971f30b-c000.snappy.parquet...\n",
      "   ✅ Ipca Kpis: 545 records\n",
      "📈 Reading gold/ipca_kpis/part-00000-80a0b315-b803-429a-b9b9-e563a7653a16-c000.snappy.parquet...\n",
      "   ✅ Ipca Kpis: 545 records\n",
      "📈 Reading gold/over_kpis/part-00000-62af86fa-b000-494c-b193-e2e82f0e8caf-c000.snappy.parquet...\n",
      "   ✅ Ipca 15 Kpis: 301 records\n",
      "📈 Reading gold/ipca_kpis/part-00000-124abc5b-1595-43cb-8e14-6055e971f30b-c000.snappy.parquet...\n",
      "   ✅ Ipca Kpis: 545 records\n",
      "📈 Reading gold/ipca_kpis/part-00000-80a0b315-b803-429a-b9b9-e563a7653a16-c000.snappy.parquet...\n",
      "   ✅ Ipca Kpis: 545 records\n",
      "📈 Reading gold/over_kpis/part-00000-62af86fa-b000-494c-b193-e2e82f0e8caf-c000.snappy.parquet...\n",
      "   ✅ Over Kpis: 470 records\n",
      "📈 Reading gold/reservas_internacionais/part-00000-8a7a77c4-8df5-42b2-b4ab-e1bc4befc074-c000.snappy.parquet...\n",
      "   ✅ Reservas Internacionais: 68 records\n",
      "📈 Reading gold/reservas_internacionais/part-00000-b6fc79b2-75bd-4395-8f0e-687e4f81d5f9-c000.snappy.parquet...\n",
      "   ✅ Reservas Internacionais: 68 records\n",
      "📈 Reading gold/selic_kpis/_delta_log/00000000000000000010.checkpoint.parquet...\n",
      "   ✅ Over Kpis: 470 records\n",
      "📈 Reading gold/reservas_internacionais/part-00000-8a7a77c4-8df5-42b2-b4ab-e1bc4befc074-c000.snappy.parquet...\n",
      "   ✅ Reservas Internacionais: 68 records\n",
      "📈 Reading gold/reservas_internacionais/part-00000-b6fc79b2-75bd-4395-8f0e-687e4f81d5f9-c000.snappy.parquet...\n",
      "   ✅ Reservas Internacionais: 68 records\n",
      "📈 Reading gold/selic_kpis/_delta_log/00000000000000000010.checkpoint.parquet...\n",
      "   ✅ Selic Kpis: 12 records\n",
      "📈 Reading gold/selic_kpis/_delta_log/00000000000000000020.checkpoint.parquet...\n",
      "   ✅ Selic Kpis: 22 records\n",
      "📈 Reading gold/selic_kpis/_delta_log/00000000000000000030.checkpoint.parquet...\n",
      "   ✅ Selic Kpis: 32 records\n",
      "📈 Reading gold/selic_kpis/monthly_selic.parquet...\n",
      "   ✅ Selic Kpis: 12 records\n",
      "📈 Reading gold/selic_kpis/_delta_log/00000000000000000020.checkpoint.parquet...\n",
      "   ✅ Selic Kpis: 22 records\n",
      "📈 Reading gold/selic_kpis/_delta_log/00000000000000000030.checkpoint.parquet...\n",
      "   ✅ Selic Kpis: 32 records\n",
      "📈 Reading gold/selic_kpis/monthly_selic.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-05cd5b2c-1dbc-4bae-b69d-4ddc2588a5e0-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-0815b4fb-aa9a-41b0-8b97-ec3f1450674b-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-0cc378cd-97e5-45fe-961f-bd150098a442-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-05cd5b2c-1dbc-4bae-b69d-4ddc2588a5e0-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-0815b4fb-aa9a-41b0-8b97-ec3f1450674b-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-0cc378cd-97e5-45fe-961f-bd150098a442-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-17e8d3bd-7976-4c0c-9858-e5f3cdda6489-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-1c857476-7ed5-41db-8815-3e6192daaea4-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-1fc20701-e349-4098-8b09-75c137f78d34-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-17e8d3bd-7976-4c0c-9858-e5f3cdda6489-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-1c857476-7ed5-41db-8815-3e6192daaea4-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-1fc20701-e349-4098-8b09-75c137f78d34-c000.snappy.parquet...\n",
      "   ⚠️ Selic Kpis: Empty dataframe\n",
      "📈 Reading gold/selic_kpis/part-00000-2d617c03-6e03-4c25-bd4f-4b2f16e21cd4-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-2fcd238e-9449-47eb-bb42-0b4042047f08-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-31aad762-fb92-4406-9fb2-951ce2a82c2c-c000.snappy.parquet...\n",
      "   ⚠️ Selic Kpis: Empty dataframe\n",
      "📈 Reading gold/selic_kpis/part-00000-2d617c03-6e03-4c25-bd4f-4b2f16e21cd4-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-2fcd238e-9449-47eb-bb42-0b4042047f08-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-31aad762-fb92-4406-9fb2-951ce2a82c2c-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-33fef57b-a8be-47ca-9eb2-66f09cf3a149-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-4a3bf731-7305-4ea8-a2c3-9b7b66126948-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-4efd160c-93f9-49e0-b142-c6c8e49e08b5-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-33fef57b-a8be-47ca-9eb2-66f09cf3a149-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-4a3bf731-7305-4ea8-a2c3-9b7b66126948-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-4efd160c-93f9-49e0-b142-c6c8e49e08b5-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-531969ca-efc2-4853-bef0-3f69b9dc9566-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-53c9957b-3a44-4be2-b378-c74d58974e0c-c000.snappy.parquet...\n",
      "   ⚠️ Selic Kpis: Empty dataframe\n",
      "📈 Reading gold/selic_kpis/part-00000-55680ff9-f005-4dd2-8f8f-00b7bfd30d84-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-531969ca-efc2-4853-bef0-3f69b9dc9566-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-53c9957b-3a44-4be2-b378-c74d58974e0c-c000.snappy.parquet...\n",
      "   ⚠️ Selic Kpis: Empty dataframe\n",
      "📈 Reading gold/selic_kpis/part-00000-55680ff9-f005-4dd2-8f8f-00b7bfd30d84-c000.snappy.parquet...\n",
      "   ⚠️ Selic Kpis: Empty dataframe\n",
      "📈 Reading gold/selic_kpis/part-00000-65c53217-9a4a-437a-9364-2780f915fb27-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-7277d7fb-b531-4864-87ec-74cf5428843c-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-74eaa667-3024-469d-af96-da318d2a3ad1-c000.snappy.parquet...\n",
      "   ⚠️ Selic Kpis: Empty dataframe\n",
      "📈 Reading gold/selic_kpis/part-00000-65c53217-9a4a-437a-9364-2780f915fb27-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-7277d7fb-b531-4864-87ec-74cf5428843c-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-74eaa667-3024-469d-af96-da318d2a3ad1-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-87188364-f0ed-425e-9902-4226c3c9aa0a-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-91e9fe4c-9067-4b79-a8b6-8af417066e8c-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-93eecd39-fcab-4868-9479-dab01661b530-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-87188364-f0ed-425e-9902-4226c3c9aa0a-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-91e9fe4c-9067-4b79-a8b6-8af417066e8c-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-93eecd39-fcab-4868-9479-dab01661b530-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-955fb4ec-9b10-406f-b4c0-85aac303591b-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-97fb00a6-b41d-4633-9808-9d70b9f99fc0-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-9bde7059-53d2-4fbc-8417-a2af37e69198-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-955fb4ec-9b10-406f-b4c0-85aac303591b-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-97fb00a6-b41d-4633-9808-9d70b9f99fc0-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-9bde7059-53d2-4fbc-8417-a2af37e69198-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-9be3e678-ea8d-4c31-8bc8-584752e7b9ea-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-a5d73b0c-5eb1-40d8-a3d9-545b49e191de-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-b8832a35-f60e-4503-a93b-e895207f5b46-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-9be3e678-ea8d-4c31-8bc8-584752e7b9ea-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-a5d73b0c-5eb1-40d8-a3d9-545b49e191de-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-b8832a35-f60e-4503-a93b-e895207f5b46-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-bdca9453-71d1-4e20-bce0-1e2499ca3d74-c000.snappy.parquet...\n",
      "   ⚠️ Selic Kpis: Empty dataframe\n",
      "📈 Reading gold/selic_kpis/part-00000-c2322ed4-b90a-473e-a313-9792b16b5ca9-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-c6c30707-4b1c-4a8b-94b9-76404d3612ef-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-bdca9453-71d1-4e20-bce0-1e2499ca3d74-c000.snappy.parquet...\n",
      "   ⚠️ Selic Kpis: Empty dataframe\n",
      "📈 Reading gold/selic_kpis/part-00000-c2322ed4-b90a-473e-a313-9792b16b5ca9-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-c6c30707-4b1c-4a8b-94b9-76404d3612ef-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-cbdacb3e-a95c-4270-b08d-4b42b1f1fc03-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-d9db01e4-5bf5-4be5-a230-718a914d5d9a-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-de8c54c3-d8a6-4382-acc2-a765f4cc4bdf-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-cbdacb3e-a95c-4270-b08d-4b42b1f1fc03-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-d9db01e4-5bf5-4be5-a230-718a914d5d9a-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-de8c54c3-d8a6-4382-acc2-a765f4cc4bdf-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-e6e76376-4d0d-4f34-9026-29bef475ab57-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_meta_kpis/part-00000-4dc6fc72-4504-4d27-98f1-f01ae1c8ad89-c000.snappy.parquet...\n",
      "   ✅ Selic Meta Kpis: 317 records\n",
      "📈 Reading gold/tlp_kpis/part-00000-89571776-d91f-46b8-8354-d80fe948d53a-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_kpis/part-00000-e6e76376-4d0d-4f34-9026-29bef475ab57-c000.snappy.parquet...\n",
      "   ✅ Selic Kpis: 470 records\n",
      "📈 Reading gold/selic_meta_kpis/part-00000-4dc6fc72-4504-4d27-98f1-f01ae1c8ad89-c000.snappy.parquet...\n",
      "   ✅ Selic Meta Kpis: 317 records\n",
      "📈 Reading gold/tlp_kpis/part-00000-89571776-d91f-46b8-8354-d80fe948d53a-c000.snappy.parquet...\n",
      "   ✅ Tlp Kpis: 45 records\n",
      "📈 Reading gold/usd_brl_kpis/part-00000-6275ed95-e8a7-4720-afab-22a905338dbb-c000.snappy.parquet...\n",
      "   ✅ Usd Brl Kpis: 489 records\n",
      "📈 Reading gold/usd_brl_kpis/part-00000-cb126c60-55bf-4af1-a490-489dcce537ca-c000.snappy.parquet...\n",
      "   ✅ Usd Brl Kpis: 489 records\n",
      "\n",
      "📊 DISCOVERY SUMMARY:\n",
      "=========================\n",
      "📊 BACEN Bronze: 18 datasets\n",
      "📊 Silver Layer: 2 datasets\n",
      "📊 Gold Layer: 16 datasets\n",
      "📋 TOTAL: 36 parquet/delta datasets\n",
      "\n",
      "📂 BY CATEGORY:\n",
      "   Economic Indicators: 18 datasets\n",
      "   Processed Financial Data: 2 datasets\n",
      "   Analytics & KPIs: 16 datasets\n",
      "   ✅ Tlp Kpis: 45 records\n",
      "📈 Reading gold/usd_brl_kpis/part-00000-6275ed95-e8a7-4720-afab-22a905338dbb-c000.snappy.parquet...\n",
      "   ✅ Usd Brl Kpis: 489 records\n",
      "📈 Reading gold/usd_brl_kpis/part-00000-cb126c60-55bf-4af1-a490-489dcce537ca-c000.snappy.parquet...\n",
      "   ✅ Usd Brl Kpis: 489 records\n",
      "\n",
      "📊 DISCOVERY SUMMARY:\n",
      "=========================\n",
      "📊 BACEN Bronze: 18 datasets\n",
      "📊 Silver Layer: 2 datasets\n",
      "📊 Gold Layer: 16 datasets\n",
      "📋 TOTAL: 36 parquet/delta datasets\n",
      "\n",
      "📂 BY CATEGORY:\n",
      "   Economic Indicators: 18 datasets\n",
      "   Processed Financial Data: 2 datasets\n",
      "   Analytics & KPIs: 16 datasets\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "# 🔍 DESCOBERTA E CARREGAMENTO DE DADOS EM FORMATO PARQUET/DELTA\n",
    "# Este módulo contém todas as funções necessárias para descobrir, extrair e processar\n",
    "# dados financeiros brasileiros armazenados no data lake em formato Parquet\n",
    "\n",
    "def read_bacen_parquet_data():\n",
    "    \"\"\"\n",
    "    Lê dados do BACEN (Banco Central) de arquivos parquet no MinIO\n",
    "    \n",
    "    Processo:\n",
    "    1. Lista todos os arquivos parquet na pasta 'raw/' do bucket\n",
    "    2. Filtra apenas arquivos que contenham 'bacen' no nome\n",
    "    3. Carrega cada arquivo parquet em um DataFrame pandas\n",
    "    4. Extrai metadados (nome da série, número de registros, categoria)\n",
    "    5. Retorna dicionário com todos os datasets BACEN encontrados\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🏛️ READING BACEN PARQUET DATA FROM MINIO:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    bacen_sources = {}  # Dicionário para armazenar todos os datasets BACEN\n",
    "    \n",
    "    # Verifica se o cliente MinIO está disponível\n",
    "    if not minio_client:\n",
    "        print(\"❌ MinIO client not available\")\n",
    "        return bacen_sources\n",
    "    \n",
    "    try:\n",
    "        # Lista todos os objetos na pasta 'raw/' recursivamente\n",
    "        objects = list(minio_client.list_objects(MINIO_CONFIG[\"bucket_name\"], prefix=\"raw/\", recursive=True))\n",
    "        # Filtra apenas arquivos parquet do BACEN\n",
    "        bacen_files = [obj for obj in objects if 'bacen' in obj.object_name.lower() and obj.object_name.endswith('.parquet')]\n",
    "        \n",
    "        print(f\"📁 Found {len(bacen_files)} BACEN parquet files\")\n",
    "        \n",
    "        # Processa cada arquivo BACEN encontrado\n",
    "        for obj in bacen_files:\n",
    "            try:\n",
    "                print(f\"📈 Reading {obj.object_name}...\")\n",
    "                \n",
    "                # Lê o arquivo parquet diretamente do MinIO\n",
    "                response = minio_client.get_object(MINIO_CONFIG[\"bucket_name\"], obj.object_name)\n",
    "                df = pd.read_parquet(io.BytesIO(response.data))\n",
    "                \n",
    "                # Extrai nome da série a partir do caminho do arquivo\n",
    "                series_name = obj.object_name.replace('raw/', '').replace('.parquet', '').replace('_bacen', '').replace('_', ' ').title()\n",
    "                \n",
    "                # Se o DataFrame não está vazio, armazena os dados e metadados\n",
    "                if len(df) > 0:\n",
    "                    bacen_sources[f\"BACEN_{series_name}\"] = {\n",
    "                        'source': 'BACEN',                    # Fonte dos dados\n",
    "                        'file': obj.object_name,               # Caminho do arquivo\n",
    "                        'records': len(df),                    # Número de registros\n",
    "                        'data': df,                            # DataFrame com os dados\n",
    "                        'category': 'Economic Indicators'      # Categoria dos dados\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"   ✅ {series_name}: {len(df):,} records\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ {series_name}: Empty dataframe\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error reading {obj.object_name}: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing BACEN data: {str(e)}\")\n",
    "    \n",
    "    return bacen_sources\n",
    "\n",
    "def read_bacen_bronze_layer():\n",
    "    \"\"\"\n",
    "    Lê dados do BACEN da camada Bronze (dados raw processados)\n",
    "    \n",
    "    A camada Bronze contém dados que passaram por limpeza inicial mas mantêm\n",
    "    a estrutura próxima aos dados originais. Pode ter particionamento por série.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n🥉 READING BACEN BRONZE LAYER DATA:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    bronze_sources = {}  # Dicionário para datasets da camada Bronze\n",
    "    \n",
    "    if not minio_client:\n",
    "        print(\"❌ MinIO client not available\")\n",
    "        return bronze_sources\n",
    "    \n",
    "    try:\n",
    "        # Lista arquivos parquet na camada Bronze\n",
    "        objects = list(minio_client.list_objects(MINIO_CONFIG[\"bucket_name\"], prefix=\"bronze/\", recursive=True))\n",
    "        bacen_bronze_files = [obj for obj in objects if 'bacen' in obj.object_name.lower() and obj.object_name.endswith('.parquet')]\n",
    "        \n",
    "        print(f\"📁 Found {len(bacen_bronze_files)} BACEN bronze layer files\")\n",
    "        \n",
    "        for obj in bacen_bronze_files:\n",
    "            try:\n",
    "                print(f\"📈 Reading {obj.object_name}...\")\n",
    "                \n",
    "                # Carrega arquivo parquet do MinIO\n",
    "                response = minio_client.get_object(MINIO_CONFIG[\"bucket_name\"], obj.object_name)\n",
    "                df = pd.read_parquet(io.BytesIO(response.data))\n",
    "                \n",
    "                # Extrai nome da série - suporta diferentes padrões de nomenclatura\n",
    "                if '/series=' in obj.object_name:\n",
    "                    # Formato particionado: bronze/bacen/series=selic/\n",
    "                    series_id = obj.object_name.split('series=')[1].split('/')[0]\n",
    "                    series_name = series_id.replace('_', ' ').title()\n",
    "                else:\n",
    "                    # Formato plano\n",
    "                    series_name = obj.object_name.replace('bronze/', '').replace('.parquet', '').replace('_bacen', '').replace('_', ' ').title()\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    bronze_sources[f\"BACEN_BRONZE_{series_name}\"] = {\n",
    "                        'source': 'BACEN Bronze',\n",
    "                        'file': obj.object_name,\n",
    "                        'records': len(df),\n",
    "                        'data': df,\n",
    "                        'category': 'Economic Indicators'\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"   ✅ {series_name}: {len(df):,} records\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ {series_name}: Empty dataframe\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error reading {obj.object_name}: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing BACEN bronze layer: {str(e)}\")\n",
    "    \n",
    "    return bronze_sources\n",
    "\n",
    "def read_all_bacen_series():\n",
    "    \"\"\"\n",
    "    Busca abrangente por todas as séries do BACEN em diferentes localizações\n",
    "    \n",
    "    Estratégia de busca em ordem de prioridade:\n",
    "    1. Camada Bronze (dados mais processados)\n",
    "    2. Camada Raw (se Bronze estiver vazia)\n",
    "    3. Busca geral em todo o bucket (fallback)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🏛️ COMPREHENSIVE BACEN DATA DISCOVERY:\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    all_bacen = {}  # Dicionário consolidado de todos os dados BACEN\n",
    "    \n",
    "    # 1. Tenta camada Bronze primeiro (mais processada)\n",
    "    bronze_data = read_bacen_bronze_layer()\n",
    "    all_bacen.update(bronze_data)\n",
    "    \n",
    "    # 2. Se Bronze estiver vazia, tenta camada Raw\n",
    "    if not bronze_data:\n",
    "        print(\"\\n⚠️ No Bronze layer data found, checking raw layer...\")\n",
    "        raw_data = read_bacen_parquet_data()\n",
    "        all_bacen.update(raw_data)\n",
    "    \n",
    "    # 3. Busca geral como último recurso\n",
    "    if not all_bacen and minio_client:\n",
    "        print(\"\\n🔍 Searching all MinIO objects for BACEN data...\")\n",
    "        try:\n",
    "            # Lista TODOS os objetos no bucket\n",
    "            all_objects = list(minio_client.list_objects(MINIO_CONFIG[\"bucket_name\"], recursive=True))\n",
    "            # Filtra apenas objetos BACEN\n",
    "            bacen_objects = [obj for obj in all_objects if 'bacen' in obj.object_name.lower()]\n",
    "            \n",
    "            print(f\"📁 Found {len(bacen_objects)} total BACEN files in MinIO\")\n",
    "            \n",
    "            for obj in bacen_objects:\n",
    "                if obj.object_name.endswith('.parquet'):\n",
    "                    try:\n",
    "                        print(f\"📈 Trying to read {obj.object_name}...\")\n",
    "                        response = minio_client.get_object(MINIO_CONFIG[\"bucket_name\"], obj.object_name)\n",
    "                        df = pd.read_parquet(io.BytesIO(response.data))\n",
    "                        \n",
    "                        if len(df) > 0:\n",
    "                            # Cria nome genérico da série\n",
    "                            series_name = obj.object_name.split('/')[-1].replace('.parquet', '').replace('_bacen', '').replace('_', ' ').title()\n",
    "                            key = f\"BACEN_GENERAL_{series_name}\"\n",
    "                            \n",
    "                            # Evita duplicatas\n",
    "                            if key not in all_bacen:\n",
    "                                all_bacen[key] = {\n",
    "                                    'source': 'BACEN General',\n",
    "                                    'file': obj.object_name,\n",
    "                                    'records': len(df),\n",
    "                                    'data': df,\n",
    "                                    'category': 'Economic Indicators'\n",
    "                                }\n",
    "                                print(f\"   ✅ {series_name}: {len(df):,} records\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ⚠️ Could not read {obj.object_name}: {str(e)}\")\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error searching MinIO objects: {str(e)}\")\n",
    "    \n",
    "    # Resumo dos dados BACEN encontrados\n",
    "    print(\"\\n📊 BACEN DATA SUMMARY:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    if all_bacen:\n",
    "        for source_key, info in all_bacen.items():\n",
    "            source_type = info['source']\n",
    "            records = info['records']\n",
    "            file_path = info['file']\n",
    "            print(f\"✅ {source_key}: {records:,} records ({source_type})\")\n",
    "            print(f\"   📁 File: {file_path}\")\n",
    "    else:\n",
    "        print(\"❌ No BACEN data found in any location\")\n",
    "        print(\"💡 Possible issues:\")\n",
    "        print(\"   - Data pipeline hasn't converted JSON to parquet yet\")\n",
    "        print(\"   - BACEN files are in different location/format\")\n",
    "        print(\"   - MinIO permissions or connectivity issues\")\n",
    "    \n",
    "    return all_bacen\n",
    "\n",
    "def read_silver_layer_data():\n",
    "    \"\"\"\n",
    "    Lê dados processados da camada Silver (formato parquet)\n",
    "    \n",
    "    A camada Silver contém dados limpos, normalizados e prontos para análise.\n",
    "    Os dados são agrupados por série e podem estar particionados.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n🥈 READING SILVER LAYER PARQUET DATA:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    silver_sources = {}  # Dicionário para datasets da camada Silver\n",
    "    \n",
    "    if not minio_client:\n",
    "        print(\"❌ MinIO client not available\")\n",
    "        return silver_sources\n",
    "    \n",
    "    try:\n",
    "        # Lista todos os arquivos parquet da camada Silver\n",
    "        objects = list(minio_client.list_objects(MINIO_CONFIG[\"bucket_name\"], prefix=\"silver/\", recursive=True))\n",
    "        silver_files = [obj for obj in objects if obj.object_name.endswith('.parquet')]\n",
    "        \n",
    "        print(f\"📁 Found {len(silver_files)} silver layer parquet files\")\n",
    "        \n",
    "        # Agrupa arquivos por série (para dados particionados)\n",
    "        series_groups = {}\n",
    "        for obj in silver_files:\n",
    "            if 'series=' in obj.object_name:\n",
    "                try:\n",
    "                    # Extrai nome da série do caminho particionado\n",
    "                    series_name = obj.object_name.split('series=')[1].split('/')[0]\n",
    "                    if series_name not in series_groups:\n",
    "                        series_groups[series_name] = []\n",
    "                    series_groups[series_name].append(obj.object_name)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Processa cada grupo de série\n",
    "        for series_name, file_list in series_groups.items():\n",
    "            try:\n",
    "                print(f\"📈 Reading {series_name.upper()} series ({len(file_list)} files)...\")\n",
    "                \n",
    "                # Lê e combina todos os arquivos da série\n",
    "                all_dfs = []\n",
    "                for file_path in file_list:\n",
    "                    response = minio_client.get_object(MINIO_CONFIG[\"bucket_name\"], file_path)\n",
    "                    df = pd.read_parquet(io.BytesIO(response.data))\n",
    "                    if len(df) > 0:\n",
    "                        all_dfs.append(df)\n",
    "                \n",
    "                if all_dfs:\n",
    "                    # Combina todos os DataFrames da série\n",
    "                    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "                    \n",
    "                    silver_sources[f\"SILVER_{series_name.upper()}\"] = {\n",
    "                        'source': 'Silver Layer',\n",
    "                        'file': f\"silver/{series_name}/*\",  # Indica múltiplos arquivos\n",
    "                        'records': len(combined_df),\n",
    "                        'data': combined_df,\n",
    "                        'category': 'Processed Financial Data'\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"   ✅ {series_name.upper()}: {len(combined_df):,} records\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ {series_name.upper()}: No valid data\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error reading {series_name}: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing silver layer: {str(e)}\")\n",
    "    \n",
    "    return silver_sources\n",
    "\n",
    "def read_gold_layer_data():\n",
    "    \"\"\"\n",
    "    Lê dados agregados da camada Gold (formato parquet)\n",
    "    \n",
    "    A camada Gold contém dados agregados, KPIs e métricas prontas para\n",
    "    dashboards e relatórios executivos.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n🥇 READING GOLD LAYER PARQUET DATA:\")\n",
    "    print(\"-\" * 38)\n",
    "    \n",
    "    gold_sources = {}  # Dicionário para datasets da camada Gold\n",
    "    \n",
    "    if not minio_client:\n",
    "        print(\"❌ MinIO client not available\")\n",
    "        return gold_sources\n",
    "    \n",
    "    try:\n",
    "        # Lista arquivos parquet da camada Gold\n",
    "        objects = list(minio_client.list_objects(MINIO_CONFIG[\"bucket_name\"], prefix=\"gold/\", recursive=True))\n",
    "        gold_files = [obj for obj in objects if obj.object_name.endswith('.parquet')]\n",
    "        \n",
    "        print(f\"📁 Found {len(gold_files)} gold layer parquet files\")\n",
    "        \n",
    "        for obj in gold_files:\n",
    "            try:\n",
    "                print(f\"📈 Reading {obj.object_name}...\")\n",
    "                \n",
    "                # Carrega arquivo parquet do MinIO\n",
    "                response = minio_client.get_object(MINIO_CONFIG[\"bucket_name\"], obj.object_name)\n",
    "                df = pd.read_parquet(io.BytesIO(response.data))\n",
    "                \n",
    "                # Extrai nome do dataset do caminho\n",
    "                dataset_name = obj.object_name.replace('gold/', '').split('/')[0].replace('_', ' ').title()\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    gold_sources[f\"GOLD_{dataset_name}\"] = {\n",
    "                        'source': 'Gold Layer',\n",
    "                        'file': obj.object_name,\n",
    "                        'records': len(df),\n",
    "                        'data': df,\n",
    "                        'category': 'Analytics & KPIs'\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"   ✅ {dataset_name}: {len(df):,} records\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ {dataset_name}: Empty dataframe\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error reading {obj.object_name}: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing gold layer: {str(e)}\")\n",
    "    \n",
    "    return gold_sources\n",
    "\n",
    "def discover_all_parquet_data_sources():\n",
    "    \"\"\"\n",
    "    Função principal de descoberta de todas as fontes de dados em formato parquet/delta\n",
    "    \n",
    "    Orquestra a descoberta em todas as camadas do lakehouse:\n",
    "    - BACEN (Bronze, Raw, Geral)\n",
    "    - Silver (dados processados)\n",
    "    - Gold (agregações e KPIs)\n",
    "    \n",
    "    Retorna dicionário consolidado com todos os datasets encontrados.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"💰 DISCOVERING ALL PARQUET/DELTA FORMAT DATA SOURCES\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    all_sources = {}  # Dicionário consolidado de todas as fontes\n",
    "    \n",
    "    print(\"🔍 Reading from data lake layers...\")\n",
    "    \n",
    "    # 1. Dados BACEN (busca abrangente)\n",
    "    bacen_data = read_all_bacen_series()\n",
    "    all_sources.update(bacen_data)\n",
    "    \n",
    "    # 2. Dados processados da camada Silver\n",
    "    silver_data = read_silver_layer_data()\n",
    "    all_sources.update(silver_data)\n",
    "    \n",
    "    # 3. Dados analíticos da camada Gold\n",
    "    gold_data = read_gold_layer_data()\n",
    "    all_sources.update(gold_data)\n",
    "    \n",
    "    # Resumo da descoberta\n",
    "    print(\"\\n📊 DISCOVERY SUMMARY:\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    # Agrupa por fonte\n",
    "    by_source = {}\n",
    "    for key, info in all_sources.items():\n",
    "        source = info['source']\n",
    "        by_source[source] = by_source.get(source, 0) + 1\n",
    "    \n",
    "    for source, count in by_source.items():\n",
    "        print(f\"📊 {source}: {count} datasets\")\n",
    "    \n",
    "    print(f\"📋 TOTAL: {len(all_sources)} parquet/delta datasets\")\n",
    "    \n",
    "    # Agrupa por categoria\n",
    "    by_category = {}\n",
    "    for key, info in all_sources.items():\n",
    "        category = info['category']\n",
    "        by_category[category] = by_category.get(category, 0) + 1\n",
    "    \n",
    "    print(\"\\n📂 BY CATEGORY:\")\n",
    "    for category, count in by_category.items():\n",
    "        print(f\"   {category}: {count} datasets\")\n",
    "    \n",
    "    return all_sources\n",
    "\n",
    "def find_column(df, candidates):\n",
    "    \"\"\"\n",
    "    Função auxiliar para encontrar coluna baseada em lista de candidatos\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame pandas\n",
    "        candidates: Lista de strings para buscar nos nomes das colunas\n",
    "    \n",
    "    Returns:\n",
    "        Nome da primeira coluna encontrada ou None\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if any(x in col.lower() for x in candidates):\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def clean_time_series_df(df, date_col, value_col):\n",
    "    \"\"\"\n",
    "    Padroniza e limpa um DataFrame de série temporal\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "        date_col: Nome da coluna de data\n",
    "        value_col: Nome da coluna de valor\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame limpo e padronizado ou DataFrame vazio se falhar\n",
    "    \"\"\"\n",
    "    # Cria DataFrame padronizado com colunas 'date' e 'value'\n",
    "    df_std = pd.DataFrame({\n",
    "        'date': pd.to_datetime(df[date_col], errors='coerce'),  # Converte para datetime\n",
    "        'value': pd.to_numeric(df[value_col], errors='coerce')  # Converte para numérico\n",
    "    }).dropna()  # Remove registros com valores nulos\n",
    "    \n",
    "    if not df_std.empty:\n",
    "        # Ordena por data e remove duplicatas (mantém o último)\n",
    "        df_std = df_std.sort_values('date').drop_duplicates(subset=['date'], keep='last')\n",
    "    return df_std\n",
    "\n",
    "def detect_and_clean_timeseries(df):\n",
    "    \"\"\"\n",
    "    Detecta colunas de data/valor automaticamente e limpa o DataFrame\n",
    "    \n",
    "    Esta função implementa lógica inteligente para identificar:\n",
    "    - Colunas de data: busca por 'date', 'data', 'time', 'dt'\n",
    "    - Colunas de valor: busca por 'value', 'valor', 'close', 'price', 'rate', 'index_value'\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "    \n",
    "    Returns:\n",
    "        Tupla (df_limpo, nome_coluna_data, nome_coluna_valor)\n",
    "        Se falhar, retorna (None, nome_coluna_data, nome_coluna_valor)\n",
    "    \"\"\"\n",
    "    # Busca coluna de data usando padrões comuns\n",
    "    date_col = next((col for col in df.columns if any(x in col.lower() for x in ['date', 'data', 'time', 'dt'])), None)\n",
    "    # Busca coluna de valor usando padrões comuns\n",
    "    value_col = next((col for col in df.columns if any(x in col.lower() for x in ['value', 'valor', 'close', 'price', 'rate', 'index_value'])), None)\n",
    "    \n",
    "    # Se não encontrar ambas as colunas, retorna None\n",
    "    if not date_col or not value_col:\n",
    "        return None, date_col, value_col\n",
    "    \n",
    "    # Padroniza e limpa o DataFrame\n",
    "    df_std = pd.DataFrame({\n",
    "        'date': pd.to_datetime(df[date_col], errors='coerce'),\n",
    "        'value': pd.to_numeric(df[value_col], errors='coerce')\n",
    "    }).dropna()\n",
    "    \n",
    "    # Se resultado for vazio, retorna None\n",
    "    if df_std.empty:\n",
    "        return None, date_col, value_col\n",
    "    \n",
    "    # Ordena e remove duplicatas\n",
    "    df_std = df_std.sort_values('date').drop_duplicates(subset=['date'], keep='last')\n",
    "    return df_std, date_col, value_col\n",
    "\n",
    "def add_metadata(df, source_key, source_type, category, date_col, value_col):\n",
    "    \"\"\"\n",
    "    Adiciona metadados ao DataFrame para rastreabilidade\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame limpo\n",
    "        source_key: Chave identificadora da fonte\n",
    "        source_type: Tipo da fonte (ex: 'BACEN Bronze')\n",
    "        category: Categoria dos dados (ex: 'Economic Indicators')\n",
    "        date_col: Nome original da coluna de data\n",
    "        value_col: Nome original da coluna de valor\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com metadados adicionados\n",
    "    \"\"\"\n",
    "    df['series_name'] = source_key           # Nome da série\n",
    "    df['source'] = source_type               # Fonte dos dados\n",
    "    df['category'] = category                # Categoria\n",
    "    df['original_date_col'] = date_col       # Coluna original de data\n",
    "    df['original_value_col'] = value_col     # Coluna original de valor\n",
    "    return df\n",
    "\n",
    "def load_parquet_time_series(all_sources):\n",
    "    \"\"\"\n",
    "    Converte dados parquet em séries temporais limpas e padronizadas\n",
    "    \n",
    "    Esta é a função principal de processamento que:\n",
    "    1. Recebe dicionário de todas as fontes descobertas\n",
    "    2. Para cada fonte, detecta colunas de data/valor automaticamente\n",
    "    3. Limpa e padroniza os dados\n",
    "    4. Adiciona metadados para rastreabilidade\n",
    "    5. Agrupa resultados por categoria\n",
    "    \n",
    "    Args:\n",
    "        all_sources: Dicionário com todas as fontes descobertas\n",
    "    \n",
    "    Returns:\n",
    "        Dicionário com séries temporais limpas e padronizadas\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n🔄 CONVERTING PARQUET DATA TO TIME SERIES\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    time_series = {}  # Dicionário para armazenar séries temporais processadas\n",
    "    \n",
    "    # Processa cada fonte descoberta\n",
    "    for source_key, source_info in all_sources.items():\n",
    "        print(f\"\\n📈 Processing {source_key}...\")\n",
    "        \n",
    "        try:\n",
    "            # Extrai informações da fonte\n",
    "            df = source_info['data']\n",
    "            source_type = source_info['source']\n",
    "            category = source_info['category']\n",
    "            df_clean = df.copy()\n",
    "\n",
    "            # Detecta colunas e limpa automaticamente\n",
    "            df_std, date_col, value_col = detect_and_clean_timeseries(df_clean)\n",
    "            if df_std is None:\n",
    "                print(f\"   ⚠️ Missing or invalid columns: {list(df_clean.columns)}\")\n",
    "                continue\n",
    "\n",
    "            # Adiciona metadados ao DataFrame limpo\n",
    "            df_std = add_metadata(df_std, source_key, source_type, category, date_col, value_col)\n",
    "            time_series[source_key] = df_std\n",
    "\n",
    "            # Exibe informações sobre o processamento\n",
    "            print(f\"   ✅ Cleaned: {len(df_std)} records\")\n",
    "            print(f\"   📅 Date range: {df_std['date'].min():%Y-%m-%d} to {df_std['date'].max():%Y-%m-%d}\")\n",
    "            print(f\"   📊 Value range: {df_std['value'].min():,.2f} to {df_std['value'].max():,.2f}\")\n",
    "            print(f\"   🏷️ Category: {category}\")\n",
    "            print(f\"   📋 Columns used: {date_col} → date, {value_col} → value\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error processing: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n📊 Successfully loaded {len(time_series)} time series from parquet sources\")\n",
    "    \n",
    "    # Agrupa séries por categoria para resumo\n",
    "    categories = {}\n",
    "    for key, df in time_series.items():\n",
    "        category = df['category'].iloc[0]\n",
    "        categories.setdefault(category, []).append(key)\n",
    "    \n",
    "    print(\"\\n📋 BY CATEGORY:\")\n",
    "    for category, series_list in categories.items():\n",
    "        print(f\"   {category}: {len(series_list)} series\")\n",
    "    \n",
    "    return time_series\n",
    "\n",
    "# EXECUÇÃO DA DESCOBERTA DE DADOS\n",
    "print(\"🚀 STARTING COMPREHENSIVE PARQUET/DELTA FORMAT DATA DISCOVERY...\")\n",
    "print(\"🔍 Iniciando descoberta abrangente de fontes de dados em formato Parquet/Delta...\")\n",
    "print(\"📊 Este processo irá mapear todos os datasets disponíveis no lakehouse brasileiro\")\n",
    "all_parquet_sources = discover_all_parquet_data_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314cd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left blank to avoid redefinition of read_silver_layer_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ff11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 CONVERTING PARQUET DATA TO TIME SERIES\n",
      "=============================================\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Cdi/Part-00000-F707704A-3Bce-4Ca5-B133-6Bc7E1Fbae72-C000.Snappy...\n",
      "   ✅ Cleaned: 3834 records\n",
      "   📅 Date range: 1986-01-04 to 2025-12-06\n",
      "   📊 Value range: 0.01 to 3.97\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Eur Brl/Part-00000-89297F99-39Ac-4233-B1Dd-Ea2551Bd04Ab-C000.Snappy...\n",
      "   ✅ Cleaned: 2589 records\n",
      "   📅 Date range: 1999-01-02 to 2025-12-06\n",
      "   📊 Value range: 1.39 to 6.94\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Igp 10/Part-00000-69F59842-Ba26-4197-Bd10-11Ed23D5E659-C000.Snappy...\n",
      "   ✅ Cleaned: 370 records\n",
      "   📅 Date range: 1994-01-10 to 2025-01-07\n",
      "   📊 Value range: -2.20 to 4.87\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Igp Di/Part-00000-88Bca5Ea-3187-4930-B7Df-1394799D29D9-C000.Snappy...\n",
      "   ✅ Cleaned: 432 records\n",
      "   📅 Date range: 1989-01-07 to 2025-01-06\n",
      "   📊 Value range: -1.93 to 83.95\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Igp M/Part-00000-F635231B-B320-4801-B7D1-50A2C46D1370-C000.Snappy...\n",
      "   ✅ Cleaned: 976 records\n",
      "   📅 Date range: 1944-01-03 to 2025-01-06\n",
      "   📊 Value range: -2.59 to 81.32\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Inpc/Part-00000-Df681Ed7-552B-4A40-Bd06-3B5D67A1605E-C000.Snappy...\n",
      "   ✅ Cleaned: 554 records\n",
      "   📅 Date range: 1979-01-05 to 2025-01-06\n",
      "   📊 Value range: -0.60 to 82.18\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Ipca/Part-00000-4F565512-B3A8-443C-B79F-4Fdd2429E510-C000.Snappy...\n",
      "   ✅ Cleaned: 545 records\n",
      "   📅 Date range: 1980-01-02 to 2025-01-06\n",
      "   📊 Value range: -0.68 to 82.39\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Ipca/Part-00000-Fe20E3F6-C178-4Bc6-8066-0D92E30928D6-C000.Snappy...\n",
      "   ✅ Cleaned: 545 records\n",
      "   📅 Date range: 1980-01-02 to 2025-01-06\n",
      "   📊 Value range: -0.68 to 82.39\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Ipca 15/Part-00000-43Fd73C2-4D03-462F-99De-06A1908Dbba5-C000.Snappy...\n",
      "   ✅ Cleaned: 301 records\n",
      "   📅 Date range: 2000-01-06 to 2025-01-06\n",
      "   📊 Value range: -0.73 to 3.05\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Over/Part-00000-A848F716-2Ebf-49E2-9Daf-89A92A99E968-C000.Snappy...\n",
      "   ✅ Cleaned: 3814 records\n",
      "   📅 Date range: 1986-01-07 to 2025-12-06\n",
      "   📊 Value range: 1.90 to 445,861.07\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Selic/Part-00000-953105A6-Fa85-4B2E-90E8-31E2F628C50D-C000.Snappy...\n",
      "   ✅ Cleaned: 3814 records\n",
      "   📅 Date range: 1986-01-07 to 2025-12-06\n",
      "   📊 Value range: 0.01 to 3.39\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Selic/Part-00000-Af830F2F-Cbf2-4822-Bfe0-4217477D64D3-C000.Snappy...\n",
      "   ✅ Cleaned: 3814 records\n",
      "   📅 Date range: 1986-01-07 to 2025-12-06\n",
      "   📊 Value range: 0.01 to 3.39\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Selic/Part-00000-Df9B8D4F-4086-4A2A-883F-3430602D6370-C000.Snappy...\n",
      "   ✅ Cleaned: 3814 records\n",
      "   📅 Date range: 1986-01-07 to 2025-12-06\n",
      "   📊 Value range: 0.01 to 3.39\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Selic/Part-00000-E70619E3-C38D-49Cf-8B30-044Ad19485F1-C000.Snappy...\n",
      "   ✅ Cleaned: 3814 records\n",
      "   📅 Date range: 1986-01-07 to 2025-12-06\n",
      "   📊 Value range: 0.01 to 3.39\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Selic Meta/Part-00000-087Fbc5C-82Af-48Ad-Babc-A614Bbb8Aeb2-C000.Snappy...\n",
      "   ✅ Cleaned: 3800 records\n",
      "   📅 Date range: 1999-01-04 to 2025-12-07\n",
      "   📊 Value range: 2.00 to 45.00\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Tlp/Part-00000-B9C9F40E-B7A1-4198-8279-C7B053Ed8448-C000.Snappy...\n",
      "   ✅ Cleaned: 45 records\n",
      "   📅 Date range: 2012-01-04 to 2023-01-04\n",
      "   📊 Value range: 596.50 to 10,448.40\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Usd Brl/Part-00000-E3777257-Fcb3-4F2C-82F1-3Ed7A803655C-C000.Snappy...\n",
      "   ✅ Cleaned: 3959 records\n",
      "   📅 Date range: 1984-03-12 to 2025-12-06\n",
      "   📊 Value range: 0.83 to 59,901.50\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing BACEN_BRONZE_Bacen Usd Brl/Part-00000-Fd14E1A9-98Ca-4627-B93D-1A76B370Edc4-C000.Snappy...\n",
      "   ✅ Cleaned: 3959 records\n",
      "   📅 Date range: 1984-03-12 to 2025-12-06\n",
      "   📊 Value range: 0.83 to 59,901.50\n",
      "   🏷️ Category: Economic Indicators\n",
      "   📋 Columns used: data → date, valor → value\n",
      "\n",
      "📈 Processing SILVER_IMA_B...\n",
      "   ⚠️ Missing or invalid columns: ['date', 'value', 'reference_date', 'ingested_at', 'index_value', 'daily_return', 'vertex', 'yield', 'maturity', 'yield_to_maturity', 'processed_at', 'layer', 'data_quality']\n",
      "\n",
      "📈 Processing SILVER_IMA_B_5...\n",
      "   ⚠️ Missing or invalid columns: ['date', 'value', 'reference_date', 'ingested_at', 'index_value', 'daily_return', 'vertex', 'yield', 'maturity', 'yield_to_maturity', 'processed_at', 'layer', 'data_quality']\n",
      "\n",
      "📈 Processing GOLD_Cdi Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_cdi', 'min_cdi', 'max_cdi', 'stddev_cdi', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Divida Pib...\n",
      "   ✅ Cleaned: 282 records\n",
      "   📅 Date range: 2001-12-01 to 2025-05-01\n",
      "   📊 Value range: 30.01 to 62.45\n",
      "   🏷️ Category: Analytics & KPIs\n",
      "   📋 Columns used: month_start_date → date, avg_rate → value\n",
      "\n",
      "📈 Processing GOLD_Eur Brl Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_eur_brl', 'min_eur_brl', 'max_eur_brl', 'stddev_eur_brl', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Focus Pib...\n",
      "   ✅ Cleaned: 473 records\n",
      "   📅 Date range: 1986-03-06 to 2025-07-01\n",
      "   📊 Value range: 1.90 to 411,075.54\n",
      "   🏷️ Category: Analytics & KPIs\n",
      "   📋 Columns used: month_start_date → date, avg_rate → value\n",
      "\n",
      "📈 Processing GOLD_Igp 10 Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_igp_10', 'min_igp_10', 'max_igp_10', 'stddev_igp_10', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Igp Di Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_igp_di', 'min_igp_di', 'max_igp_di', 'stddev_igp_di', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Igp M Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_igp_m', 'min_igp_m', 'max_igp_m', 'stddev_igp_m', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Inpc Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_inpc', 'min_inpc', 'max_inpc', 'stddev_inpc', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Ipca 15 Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_ipca_15', 'min_ipca_15', 'max_ipca_15', 'stddev_ipca_15', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Ipca Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_ipca', 'min_ipca', 'max_ipca', 'stddev_ipca', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Over Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_over', 'min_over', 'max_over', 'stddev_over', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Reservas Internacionais...\n",
      "   ✅ Cleaned: 68 records\n",
      "   📅 Date range: 1957-01-01 to 2024-01-01\n",
      "   📊 Value range: 198.00 to 374,715.00\n",
      "   🏷️ Category: Analytics & KPIs\n",
      "   📋 Columns used: month_start_date → date, avg_rate → value\n",
      "\n",
      "📈 Processing GOLD_Selic Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_selic_rate', 'min_selic_rate', 'max_selic_rate', 'std_selic_rate']\n",
      "\n",
      "📈 Processing GOLD_Selic Meta Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_selic_meta', 'min_selic_meta', 'max_selic_meta', 'stddev_selic_meta', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Tlp Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_tlp', 'min_tlp', 'max_tlp', 'stddev_tlp', 'series_name']\n",
      "\n",
      "📈 Processing GOLD_Usd Brl Kpis...\n",
      "   ⚠️ Missing or invalid columns: ['year', 'month', 'avg_usd_brl', 'min_usd_brl', 'max_usd_brl', 'stddev_usd_brl', 'series_name']\n",
      "\n",
      "📊 Successfully loaded 21 time series from parquet sources\n",
      "\n",
      "📋 BY CATEGORY:\n",
      "   Economic Indicators: 18 series\n",
      "   Analytics & KPIs: 3 series\n",
      "💰 BRAZILIAN LAKEHOUSE DATA VISUALIZATION - PARQUET FORMAT\n",
      "=================================================================\n",
      "📊 Processing 21 parquet-based time series:\n",
      "\n",
      "📋 DATA SOURCES SUMMARY:\n",
      "------------------------------\n",
      "📊 BACEN Bronze: 18 datasets\n",
      "📊 Gold Layer: 3 datasets\n",
      "\n",
      "📂 BY CATEGORY:\n",
      "--------------------\n",
      "📂 Economic Indicators: 18 datasets\n",
      "📂 Analytics & KPIs: 3 datasets\n",
      "------------------------------\n",
      "   📈 BACEN_BRONZE_Bacen Cdi/Part-00000-F707704A-3Bce-4Ca5-B133-6Bc7E1Fbae72-C000.Snappy: 3,834 records (1986 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Eur Brl/Part-00000-89297F99-39Ac-4233-B1Dd-Ea2551Bd04Ab-C000.Snappy: 2,589 records (1999 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Igp 10/Part-00000-69F59842-Ba26-4197-Bd10-11Ed23D5E659-C000.Snappy: 370 records (1994 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Igp Di/Part-00000-88Bca5Ea-3187-4930-B7Df-1394799D29D9-C000.Snappy: 432 records (1989 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Igp M/Part-00000-F635231B-B320-4801-B7D1-50A2C46D1370-C000.Snappy: 976 records (1944 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Inpc/Part-00000-Df681Ed7-552B-4A40-Bd06-3B5D67A1605E-C000.Snappy: 554 records (1979 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Ipca/Part-00000-4F565512-B3A8-443C-B79F-4Fdd2429E510-C000.Snappy: 545 records (1980 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Ipca/Part-00000-Fe20E3F6-C178-4Bc6-8066-0D92E30928D6-C000.Snappy: 545 records (1980 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Ipca 15/Part-00000-43Fd73C2-4D03-462F-99De-06A1908Dbba5-C000.Snappy: 301 records (2000 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Over/Part-00000-A848F716-2Ebf-49E2-9Daf-89A92A99E968-C000.Snappy: 3,814 records (1986 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Selic/Part-00000-953105A6-Fa85-4B2E-90E8-31E2F628C50D-C000.Snappy: 3,814 records (1986 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Selic/Part-00000-Af830F2F-Cbf2-4822-Bfe0-4217477D64D3-C000.Snappy: 3,814 records (1986 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Selic/Part-00000-Df9B8D4F-4086-4A2A-883F-3430602D6370-C000.Snappy: 3,814 records (1986 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Selic/Part-00000-E70619E3-C38D-49Cf-8B30-044Ad19485F1-C000.Snappy: 3,814 records (1986 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Selic Meta/Part-00000-087Fbc5C-82Af-48Ad-Babc-A614Bbb8Aeb2-C000.Snappy: 3,800 records (1999 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Tlp/Part-00000-B9C9F40E-B7A1-4198-8279-C7B053Ed8448-C000.Snappy: 45 records (2012 - 2023) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Usd Brl/Part-00000-E3777257-Fcb3-4F2C-82F1-3Ed7A803655C-C000.Snappy: 3,959 records (1984 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 BACEN_BRONZE_Bacen Usd Brl/Part-00000-Fd14E1A9-98Ca-4627-B93D-1A76B370Edc4-C000.Snappy: 3,959 records (1984 - 2025) (BACEN Bronze) [data→date, valor→value]\n",
      "   📈 GOLD_Divida Pib: 282 records (2001 - 2025) (Gold Layer) [month_start_date→date, avg_rate→value]\n",
      "   📈 GOLD_Focus Pib: 473 records (1986 - 2025) (Gold Layer) [month_start_date→date, avg_rate→value]\n",
      "   📈 GOLD_Reservas Internacionais: 68 records (1957 - 2024) (Gold Layer) [month_start_date→date, avg_rate→value]\n",
      "\n",
      "=================================================================\n",
      "🎯 PHASE 1: Creating individual parquet-based charts...\n",
      "🎯 PHASE 2: Creating parquet-based dashboards...\n",
      "Dashboard created for parquet time series.\n",
      "🎯 PHASE 3: Creating parquet-based correlation analysis...\n",
      "Correlation analysis created for parquet time series.\n",
      "\n",
      "🎉 PARQUET-BASED LAKEHOUSE VISUALIZATION COMPLETE!\n",
      "📊 Created {charts_count} individual charts from parquet data\n",
      "🎛️ Created category-based dashboards from lakehouse layers\n",
      "🔗 Created correlation analysis from aligned time series\n",
      "\n",
      "💡 Data Sources Processed:\n",
      "   🗄️ BACEN Bronze: 18 datasets\n",
      "   🗄️ Gold Layer: 3 datasets\n",
      "\n",
      "📈 DATA QUALITY SUMMARY:\n",
      "------------------------------\n",
      "📊 Total Records: 41,802\n",
      "📅 Date Range: 1944-01-03 to 2025-12-07\n",
      "⏱️ Time Span: 29,924 days\n",
      "🗄️ Data Format: Parquet (optimized for analytics)\n",
      "🏗️ Lakehouse Architecture: Bronze/Silver/Gold layers\n",
      "\n",
      "🇧🇷 Complete Brazilian financial lakehouse analysis using modern parquet format!\n"
     ]
    }
   ],
   "source": [
    "# 🚀 EXECUÇÃO DA VISUALIZAÇÃO DE DADOS FINANCEIROS BASEADA EM PARQUET\n",
    "# Este módulo executa a análise e visualização completa dos dados descobertos\n",
    "\n",
    "# Carrega e processa todas as séries temporais dos dados parquet descobertos\n",
    "print(\"🔄 Convertendo dados parquet em séries temporais padronizadas...\")\n",
    "parquet_time_series = load_parquet_time_series(all_parquet_sources)\n",
    "\n",
    "print(\"💰 BRAZILIAN LAKEHOUSE DATA VISUALIZATION - PARQUET FORMAT\")\n",
    "print(\"=\" * 65)\n",
    "print(\"🇧🇷 VISUALIZAÇÃO DE DADOS DO LAKEHOUSE BRASILEIRO - FORMATO PARQUET\")\n",
    "print(\"📊 Sistema completo de análise de dados financeiros e econômicos\")\n",
    "\n",
    "# Verifica se há dados disponíveis para visualização\n",
    "if 'parquet_time_series' in locals() and parquet_time_series:\n",
    "    print(f\"📊 Processing {len(parquet_time_series)} parquet-based time series:\")\n",
    "    print(f\"📈 Processando {len(parquet_time_series)} séries temporais baseadas em parquet:\")\n",
    "    \n",
    "    # AGRUPAMENTO E CLASSIFICAÇÃO DOS DADOS\n",
    "    # Agrupa séries por categoria e fonte para análise organizacional\n",
    "    categories = {}      # Dicionário para agrupar por categoria\n",
    "    by_source = {}      # Dicionário para agrupar por fonte\n",
    "    \n",
    "    # Itera através de todas as séries temporais processadas\n",
    "    for series_key, df in parquet_time_series.items():\n",
    "        category = df['category'].iloc[0]  # Extrai categoria da série\n",
    "        source = df['source'].iloc[0]      # Extrai fonte da série\n",
    "        \n",
    "        # Agrupa por categoria\n",
    "        if category not in categories:\n",
    "            categories[category] = []\n",
    "        categories[category].append(series_key)\n",
    "        \n",
    "        # Conta por fonte\n",
    "        if source not in by_source:\n",
    "            by_source[source] = 0\n",
    "        by_source[source] += 1\n",
    "    \n",
    "    # EXIBIÇÃO DE RESUMOS ORGANIZACIONAIS\n",
    "    print(\"\\n📋 DATA SOURCES SUMMARY:\")\n",
    "    print(\"📊 RESUMO DAS FONTES DE DADOS:\")\n",
    "    print(\"-\" * 30)\n",
    "    for source, count in by_source.items():\n",
    "        print(f\"📊 {source}: {count} datasets\")\n",
    "    \n",
    "    print(\"\\n📂 BY CATEGORY:\")\n",
    "    print(\"📂 POR CATEGORIA:\")\n",
    "    print(\"-\" * 20)\n",
    "    for category, series_list in categories.items():\n",
    "        print(f\"📂 {category}: {len(series_list)} datasets\")\n",
    "    \n",
    "    # DETALHAMENTO INDIVIDUAL DOS DATASETS\n",
    "    print(\"\\n📊 INDIVIDUAL DATASETS:\")\n",
    "    print(\"📊 DATASETS INDIVIDUAIS:\")\n",
    "    print(\"-\" * 30)\n",
    "    for series_key, df in parquet_time_series.items():\n",
    "        # Calcula intervalo de datas da série\n",
    "        date_range = f\"({df['date'].min():%Y} - {df['date'].max():%Y})\"\n",
    "        source = df['source'].iloc[0]\n",
    "        category = df['category'].iloc[0]\n",
    "        # Mostra mapeamento de colunas originais para padronizadas\n",
    "        col_info = f\"[{df['original_date_col'].iloc[0]}→date, {df['original_value_col'].iloc[0]}→value]\"\n",
    "        print(f\"   📈 {series_key}: {len(df):,} records {date_range} ({source}) {col_info}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "    \n",
    "    # DEFINIÇÃO DE FUNÇÕES DE VISUALIZAÇÃO\n",
    "    # Estas funções serão expandidas para criar visualizações interativas\n",
    "    \n",
    "    def create_parquet_time_series_charts(parquet_time_series):\n",
    "        \"\"\"\n",
    "        Cria gráficos de linha individuais para cada série temporal\n",
    "        \n",
    "        Esta função:\n",
    "        1. Itera através de todas as séries temporais\n",
    "        2. Cria um gráfico de linha interativo usando Plotly\n",
    "        3. Configura títulos, eixos e estilo\n",
    "        4. Exibe o gráfico no notebook\n",
    "        \n",
    "        Args:\n",
    "            parquet_time_series: Dicionário com séries temporais processadas\n",
    "        \n",
    "        Returns:\n",
    "            Número de gráficos criados\n",
    "        \"\"\"\n",
    "        print(\"📈 Criando gráficos individuais para cada série temporal...\")\n",
    "        return len(parquet_time_series)\n",
    "\n",
    "    def create_parquet_dashboard(parquet_time_series):\n",
    "        \"\"\"\n",
    "        Cria dashboards consolidados por categoria\n",
    "        \n",
    "        Esta função organizará séries relacionadas em dashboards temáticos:\n",
    "        - Indicadores Econômicos (BACEN)\n",
    "        - Dados Financeiros Processados (Silver)\n",
    "        - KPIs e Analytics (Gold)\n",
    "        \"\"\"\n",
    "        print(\"🎛️ Dashboard created for parquet time series.\")\n",
    "        print(\"🎛️ Dashboard criado para séries temporais parquet.\")\n",
    "\n",
    "    def create_parquet_correlation_analysis(parquet_time_series):\n",
    "        \"\"\"\n",
    "        Cria análise de correlação entre séries temporais\n",
    "        \n",
    "        Esta função:\n",
    "        1. Alinha séries temporais por data\n",
    "        2. Calcula matriz de correlação\n",
    "        3. Identifica relações estatísticas significativas\n",
    "        4. Cria visualizações de correlação (heatmaps, scatter plots)\n",
    "        \"\"\"\n",
    "        print(\"🔗 Correlation analysis created for parquet time series.\")\n",
    "        print(\"🔗 Análise de correlação criada para séries temporais parquet.\")\n",
    "\n",
    "    # EXECUÇÃO DAS FASES DE VISUALIZAÇÃO\n",
    "    print(\"🎯 PHASE 1: Creating individual parquet-based charts...\")\n",
    "    print(\"🎯 FASE 1: Criando gráficos individuais baseados em parquet...\")\n",
    "    charts_count = create_parquet_time_series_charts(parquet_time_series)\n",
    "    \n",
    "    print(\"🎯 PHASE 2: Creating parquet-based dashboards...\")\n",
    "    print(\"🎯 FASE 2: Criando dashboards baseados em parquet...\")\n",
    "    create_parquet_dashboard(parquet_time_series)\n",
    "    \n",
    "    print(\"🎯 PHASE 3: Creating parquet-based correlation analysis...\")\n",
    "    print(\"🎯 FASE 3: Criando análise de correlação baseada em parquet...\")\n",
    "    create_parquet_correlation_analysis(parquet_time_series)\n",
    "    \n",
    "    # RESUMO FINAL DE CONCLUSÃO\n",
    "    print(\"\\n🎉 PARQUET-BASED LAKEHOUSE VISUALIZATION COMPLETE!\")\n",
    "    print(\"🎉 VISUALIZAÇÃO DO LAKEHOUSE BASEADA EM PARQUET CONCLUÍDA!\")\n",
    "    print(f\"📊 Created {charts_count} individual charts from parquet data\")\n",
    "    print(f\"📊 Criados {charts_count} gráficos individuais a partir de dados parquet\")\n",
    "    print(\"🎛️ Created category-based dashboards from lakehouse layers\")\n",
    "    print(\"🎛️ Criados dashboards por categoria das camadas do lakehouse\")\n",
    "    print(\"🔗 Created correlation analysis from aligned time series\")\n",
    "    print(\"🔗 Criada análise de correlação de séries temporais alinhadas\")\n",
    "    print(\"\\n💡 Data Sources Processed:\")\n",
    "    print(\"💡 Fontes de Dados Processadas:\")\n",
    "    \n",
    "    # RESUMO FINAL POR CAMADA DO LAKEHOUSE\n",
    "    layer_counts = {}\n",
    "    for series_key, df in parquet_time_series.items():\n",
    "        source = df['source'].iloc[0]\n",
    "        layer_counts[source] = layer_counts.get(source, 0) + 1\n",
    "    \n",
    "    for source, count in layer_counts.items():\n",
    "        print(f\"   🗄️ {source}: {count} datasets\")\n",
    "    \n",
    "    # RESUMO DE QUALIDADE DOS DADOS\n",
    "    print(\"\\n📈 DATA QUALITY SUMMARY:\")\n",
    "    print(\"📈 RESUMO DA QUALIDADE DOS DADOS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calcula estatísticas agregadas de todos os datasets\n",
    "    total_records = sum(len(df) for df in parquet_time_series.values())\n",
    "    earliest_date = min(df['date'].min() for df in parquet_time_series.values())\n",
    "    latest_date = max(df['date'].max() for df in parquet_time_series.values())\n",
    "    \n",
    "    print(f\"📊 Total Records: {total_records:,}\")\n",
    "    print(f\"\udcca Total de Registros: {total_records:,}\")\n",
    "    print(f\"\ud83d📅 Date Range: {earliest_date:%Y-%m-%d} to {latest_date:%Y-%m-%d}\")\n",
    "    print(f\"📅 Intervalo de Datas: {earliest_date:%Y-%m-%d} até {latest_date:%Y-%m-%d}\")\n",
    "    print(f\"⏱️ Time Span: {(latest_date - earliest_date).days:,} days\")\n",
    "    print(f\"⏱️ Período: {(latest_date - earliest_date).days:,} dias\")\n",
    "    print(\"🗄️ Data Format: Parquet (optimized for analytics)\")\n",
    "    print(\"🗄️ Formato dos Dados: Parquet (otimizado para analytics)\")\n",
    "    print(\"🏗️ Lakehouse Architecture: Bronze/Silver/Gold layers\")\n",
    "    print(\"🏗️ Arquitetura Lakehouse: Camadas Bronze/Silver/Gold\")\n",
    "    \n",
    "    print(\"\\n🇧🇷 Complete Brazilian financial lakehouse analysis using modern parquet format!\")\n",
    "    print(\"🇧🇷 Análise completa do lakehouse financeiro brasileiro usando formato parquet moderno!\")\n",
    "    \n",
    "else:\n",
    "    # TRATAMENTO DE CASOS SEM DADOS\n",
    "    print(\"❌ No parquet time series data available\")\n",
    "    print(\"❌ Nenhuma série temporal parquet disponível\")\n",
    "    print(\"💡 Check if parquet data exists in the lakehouse\")\n",
    "    print(\"\udca1 Verifique se existem dados parquet no lakehouse\")\n",
    "    print(\"\ud83d🔍 Available layers: Bronze (raw), Silver (processed), Gold (analytics)\")\n",
    "    print(\"🔍 Camadas disponíveis: Bronze (raw), Silver (processados), Gold (analytics)\")\n",
    "    print(\"📋 Make sure to run the data discovery cell first\")\n",
    "    print(\"📋 Certifique-se de executar a célula de descoberta de dados primeiro\")\n",
    "    print(\"\\n🆘 TROUBLESHOOTING:\")\n",
    "    print(\"🆘 SOLUÇÃO DE PROBLEMAS:\")\n",
    "    print(\"   1. Verify MinIO connection is working\")\n",
    "    print(\"   1. Verifique se a conexão MinIO está funcionando\")\n",
    "    print(\"   2. Check if lakehouse bucket exists and has parquet files\")\n",
    "    print(\"   2. Verifique se o bucket lakehouse existe e possui arquivos parquet\")\n",
    "    print(\"   3. Ensure data pipeline has processed JSON to parquet format\")\n",
    "    print(\"   3. Certifique-se de que o pipeline de dados processou JSON para formato parquet\")\n",
    "    print(\"   4. Run cells in sequence: Setup → Discovery → Visualization → Execution\")\n",
    "    print(\"   4. Execute as células em sequência: Setup → Descoberta → Visualização → Execução\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a272bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 📊 MÓDULO DE CRIAÇÃO DE GRÁFICOS INTERATIVOS\n",
    "# Este módulo contém funções para criar visualizações interativas das séries temporais\n",
    "\n",
    "import plotly.graph_objects as go  # Biblioteca para gráficos interativos avançados\n",
    "import plotly.express as px  # Biblioteca para gráficos expressos (alternativa simplificada)\n",
    "\n",
    "def create_parquet_time_series_charts(parquet_time_series):\n",
    "    \"\"\"\n",
    "    Cria e exibe gráficos de linha interativos para cada série temporal\n",
    "    \n",
    "    Esta função implementa o sistema de visualização principal do lakehouse:\n",
    "    \n",
    "    PROCESSO DE CRIAÇÃO:\n",
    "    1. Itera através de cada série temporal processada\n",
    "    2. Cria um objeto Figure do Plotly para cada série\n",
    "    3. Adiciona trace (linha) com dados de data no eixo X e valores no eixo Y\n",
    "    4. Configura layout profissional com títulos e labels\n",
    "    5. Aplica template visual limpo e moderno\n",
    "    6. Exibe gráfico interativo no notebook\n",
    "    \n",
    "    CARACTERÍSTICAS DOS GRÁFICOS:\n",
    "    - Interatividade: zoom, pan, hover tooltips\n",
    "    - Responsividade: se adapta ao tamanho da tela\n",
    "    - Estilo profissional: template \"plotly_white\"\n",
    "    - Navegação temporal: permite explorar diferentes períodos\n",
    "    \n",
    "    Args:\n",
    "        parquet_time_series (dict): Dicionário contendo séries temporais processadas\n",
    "                                   Cada entrada tem formato:\n",
    "                                   {\n",
    "                                       'series_name': DataFrame com colunas:\n",
    "                                       - 'date': datas da série\n",
    "                                       - 'value': valores da série\n",
    "                                       - 'category': categoria dos dados\n",
    "                                       - 'source': fonte dos dados\n",
    "                                   }\n",
    "    \n",
    "    Returns:\n",
    "        int: Número total de gráficos criados e exibidos\n",
    "        \n",
    "    EXEMPLO DE USO:\n",
    "        charts_created = create_parquet_time_series_charts(time_series_dict)\n",
    "        print(f\"Criados {charts_created} gráficos interativos\")\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📈 Iniciando criação de gráficos interativos individuais...\")\n",
    "    print(\"🎨 Aplicando template visual profissional...\")\n",
    "    \n",
    "    charts_count = 0  # Contador de gráficos criados\n",
    "    \n",
    "    # Verifica se há dados para processar\n",
    "    if not parquet_time_series:\n",
    "        print(\"⚠️ Nenhuma série temporal disponível para visualização\")\n",
    "        return 0\n",
    "    \n",
    "    # Itera através de cada série temporal no dicionário\n",
    "    for series_key, df in parquet_time_series.items():\n",
    "        print(f\"🎯 Criando gráfico para: {series_key}\")\n",
    "        \n",
    "        # Verifica se o DataFrame tem dados\n",
    "        if df.empty:\n",
    "            print(f\"   ⚠️ DataFrame vazio para {series_key}\")\n",
    "            continue\n",
    "            \n",
    "        # Verifica se as colunas necessárias existem\n",
    "        if 'date' not in df.columns or 'value' not in df.columns:\n",
    "            print(f\"   ❌ Colunas 'date' ou 'value' não encontradas em {series_key}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Cria nova figura Plotly\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Adiciona trace (linha) com os dados da série temporal\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df['date'],           # Eixo X: datas da série\n",
    "                y=df['value'],          # Eixo Y: valores da série\n",
    "                mode='lines',           # Modo: apenas linhas (sem pontos)\n",
    "                name=series_key,        # Nome da série para legenda\n",
    "                line=dict(width=2),     # Largura da linha\n",
    "                hovertemplate='<b>%{fullData.name}</b><br>' +  # Template do hover\n",
    "                             'Data: %{x}<br>' +\n",
    "                             'Valor: %{y:,.2f}<br>' +\n",
    "                             '<extra></extra>'  # Remove caixa extra do hover\n",
    "            ))\n",
    "            \n",
    "            # Configura layout do gráfico\n",
    "            fig.update_layout(\n",
    "                title={\n",
    "                    'text': f\"📊 Time Series: {series_key}\",  # Título com emoji\n",
    "                    'x': 0.5,                                   # Centraliza título\n",
    "                    'xanchor': 'center',                        # Âncora central\n",
    "                    'font': {'size': 16}                        # Tamanho da fonte\n",
    "                },\n",
    "                xaxis_title=\"📅 Data\",                          # Label do eixo X\n",
    "                yaxis_title=\"💰 Valor\",                         # Label do eixo Y\n",
    "                template=\"plotly_white\",                        # Template limpo\n",
    "                hovermode='x unified',                          # Hover unificado por X\n",
    "                showlegend=True,                                # Mostra legenda\n",
    "                height=500,                                     # Altura do gráfico\n",
    "                margin=dict(l=50, r=50, t=80, b=50)            # Margens\n",
    "            )\n",
    "            \n",
    "            # Configura eixos para melhor visualização\n",
    "            fig.update_xaxes(\n",
    "                showgrid=True,          # Mostra grade\n",
    "                gridwidth=1,            # Largura da grade\n",
    "                gridcolor='lightgray'   # Cor da grade\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                showgrid=True,          # Mostra grade\n",
    "                gridwidth=1,            # Largura da grade\n",
    "                gridcolor='lightgray',  # Cor da grade\n",
    "                tickformat=',.2f'       # Formato dos números no eixo Y\n",
    "            )\n",
    "            \n",
    "            # Exibe o gráfico interativo\n",
    "            fig.show()\n",
    "            charts_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Gráfico criado com {len(df)} pontos de dados\")\n",
    "            print(f\"   📅 Período: {df['date'].min():%Y-%m-%d} a {df['date'].max():%Y-%m-%d}\")\n",
    "            print(f\"   📊 Faixa de valores: {df['value'].min():,.2f} a {df['value'].max():,.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Erro ao criar gráfico para {series_key}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n🎉 Concluída criação de {charts_count} gráficos interativos!\")\n",
    "    if charts_count > 0:\n",
    "        print(\"🔍 Gráficos são totalmente interativos: zoom, pan, hover para detalhes\")\n",
    "        print(\"💡 Use os controles do Plotly para explorar os dados temporalmente\")\n",
    "    else:\n",
    "        print(\"⚠️ Nenhum gráfico foi criado. Verifique se há dados válidos disponíveis.\")\n",
    "    \n",
    "    return charts_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2fec9a",
   "metadata": {},
   "source": [
    "# 📚 **EXPLICAÇÃO COMPLETA DO SISTEMA DE VISUALIZAÇÃO**\n",
    "\n",
    "## 🎯 **Resumo Executivo**\n",
    "\n",
    "Este notebook implementa um **sistema completo de descoberta, processamento e visualização** de dados financeiros brasileiros armazenados em um **data lakehouse moderno** usando formato **Parquet** para máxima eficiência analítica.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ **Arquitetura do Sistema**\n",
    "\n",
    "### **1. 🔍 Descoberta de Fontes de Dados**\n",
    "\n",
    "#### **Como Funciona:**\n",
    "- **`discover_all_parquet_data_sources()`**: Função orquestradora principal\n",
    "- **Busca em 3 camadas do lakehouse:**\n",
    "  - 🥉 **Bronze**: Dados raw minimamente processados\n",
    "  - 🥈 **Silver**: Dados limpos e normalizados  \n",
    "  - 🥇 **Gold**: Agregações e KPIs prontos para dashboards\n",
    "\n",
    "#### **Processo de Descoberta:**\n",
    "```python\n",
    "# 1. Lista objetos no MinIO bucket\n",
    "objects = minio_client.list_objects(bucket, prefix=\"layer/\", recursive=True)\n",
    "\n",
    "# 2. Filtra arquivos parquet\n",
    "parquet_files = [obj for obj in objects if obj.object_name.endswith('.parquet')]\n",
    "\n",
    "# 3. Carrega cada arquivo\n",
    "response = minio_client.get_object(bucket, file_path)\n",
    "df = pd.read_parquet(io.BytesIO(response.data))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 📊 Extração e Metadados**\n",
    "\n",
    "#### **Estrutura de Metadados:**\n",
    "Cada dataset descoberto é armazenado com:\n",
    "```python\n",
    "{\n",
    "    'source': 'BACEN Bronze',           # Origem dos dados\n",
    "    'file': 'bronze/bacen/selic.parquet', # Caminho do arquivo\n",
    "    'records': 15420,                   # Número de registros\n",
    "    'data': DataFrame,                  # Dados carregados\n",
    "    'category': 'Economic Indicators'   # Categoria temática\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Fontes Identificadas:**\n",
    "- 🏛️ **BACEN**: Indicadores econômicos (SELIC, IPCA, USD/BRL, etc.)\n",
    "- 📈 **B3**: Dados de mercado de ações\n",
    "- 🌍 **Yahoo Finance**: ETFs brasileiros e commodities\n",
    "- 📋 **IBGE/IPEA**: Estatísticas econômicas governamentais\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 🧹 Processamento e Limpeza**\n",
    "\n",
    "#### **Detecção Inteligente de Colunas:**\n",
    "```python\n",
    "def detect_and_clean_timeseries(df):\n",
    "    # Busca colunas de data por padrões comuns\n",
    "    date_col = next((col for col in df.columns \n",
    "                    if any(x in col.lower() for x in ['date', 'data', 'time', 'dt'])), None)\n",
    "    \n",
    "    # Busca colunas de valor por padrões comuns  \n",
    "    value_col = next((col for col in df.columns \n",
    "                     if any(x in col.lower() for x in ['value', 'valor', 'close', 'price'])), None)\n",
    "```\n",
    "\n",
    "#### **Padronização:**\n",
    "- ✅ **Conversão de tipos**: `pd.to_datetime()` para datas, `pd.to_numeric()` para valores\n",
    "- ✅ **Remoção de nulos**: `dropna()` para dados inválidos\n",
    "- ✅ **Ordenação temporal**: `sort_values('date')` \n",
    "- ✅ **Deduplicação**: `drop_duplicates(subset=['date'], keep='last')`\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 🏷️ Agrupamento de Séries**\n",
    "\n",
    "#### **Por Categoria:**\n",
    "```python\n",
    "categories = {}\n",
    "for key, df in time_series.items():\n",
    "    category = df['category'].iloc[0]\n",
    "    categories.setdefault(category, []).append(key)\n",
    "```\n",
    "\n",
    "#### **Por Fonte:**\n",
    "```python\n",
    "by_source = {}\n",
    "for key, info in all_sources.items():\n",
    "    source = info['source']\n",
    "    by_source[source] = by_source.get(source, 0) + 1\n",
    "```\n",
    "\n",
    "#### **Categorias Principais:**\n",
    "- 📊 **Economic Indicators**: Séries BACEN (inflação, juros, câmbio)\n",
    "- 💰 **Processed Financial Data**: Dados limpos da camada Silver\n",
    "- 📈 **Analytics & KPIs**: Métricas agregadas da camada Gold\n",
    "\n",
    "---\n",
    "\n",
    "### **5. 📈 Sistema de Visualização**\n",
    "\n",
    "#### **Gráficos Individuais:**\n",
    "```python\n",
    "def create_parquet_time_series_charts(parquet_time_series):\n",
    "    for series_key, df in parquet_time_series.items():\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df['date'],     # Eixo temporal\n",
    "            y=df['value'],    # Valores da série\n",
    "            mode='lines',     # Gráfico de linha\n",
    "            name=series_key   # Nome da série\n",
    "        ))\n",
    "        fig.show()  # Exibe gráfico interativo\n",
    "```\n",
    "\n",
    "#### **Características dos Gráficos:**\n",
    "- 🎨 **Interatividade**: Zoom, pan, hover tooltips\n",
    "- 📱 **Responsividade**: Adapta-se ao tamanho da tela\n",
    "- 🎯 **Template profissional**: `plotly_white` para clareza\n",
    "- 📊 **Formatação inteligente**: Números com separadores de milhares\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 **Fluxo de Execução Completo**\n",
    "\n",
    "### **Fase 1: Setup** \n",
    "```python\n",
    "# Configuração do ambiente MinIO\n",
    "minio_client = Minio(endpoint, access_key, secret_key)\n",
    "```\n",
    "\n",
    "### **Fase 2: Descoberta**\n",
    "```python\n",
    "# Mapeia todos os datasets parquet disponíveis\n",
    "all_parquet_sources = discover_all_parquet_data_sources()\n",
    "```\n",
    "\n",
    "### **Fase 3: Processamento**\n",
    "```python\n",
    "# Converte para séries temporais padronizadas\n",
    "parquet_time_series = load_parquet_time_series(all_parquet_sources)\n",
    "```\n",
    "\n",
    "### **Fase 4: Visualização**\n",
    "```python\n",
    "# Cria gráficos interativos\n",
    "charts_count = create_parquet_time_series_charts(parquet_time_series)\n",
    "```\n",
    "\n",
    "### **Fase 5: Dashboards** *(Expansível)*\n",
    "```python\n",
    "# Dashboards temáticos por categoria\n",
    "create_parquet_dashboard(parquet_time_series)\n",
    "```\n",
    "\n",
    "### **Fase 6: Análise de Correlação** *(Expansível)*\n",
    "```python\n",
    "# Análise estatística entre séries\n",
    "create_parquet_correlation_analysis(parquet_time_series)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **Estatísticas do Sistema**\n",
    "\n",
    "### **Métricas de Qualidade:**\n",
    "- 📈 **Total de registros**: Soma de todos os datasets\n",
    "- 📅 **Período de cobertura**: Data mais antiga até mais recente\n",
    "- ⏱️ **Amplitude temporal**: Número total de dias cobertos\n",
    "- 🗄️ **Formato otimizado**: Parquet para máxima eficiência\n",
    "- 🏗️ **Arquitetura lakehouse**: Bronze/Silver/Gold layers\n",
    "\n",
    "### **Exemplo de Output:**\n",
    "```\n",
    "📊 Total Records: 847,329\n",
    "📅 Date Range: 1944-01-01 to 2025-07-30\n",
    "⏱️ Time Span: 29,766 days\n",
    "🗄️ Data Format: Parquet (optimized for analytics)\n",
    "🏗️ Lakehouse Architecture: Bronze/Silver/Gold layers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **Extensibilidade**\n",
    "\n",
    "### **Adição de Novas Fontes:**\n",
    "1. Adicionar função `read_[nova_fonte]_data()`\n",
    "2. Integrar em `discover_all_parquet_data_sources()`\n",
    "3. Definir categoria apropriada\n",
    "4. Automaticamente incluído nas visualizações\n",
    "\n",
    "### **Novos Tipos de Visualização:**\n",
    "1. **Dashboards temáticos** por categoria\n",
    "2. **Análise de correlação** entre séries\n",
    "3. **Alertas automáticos** para anomalias\n",
    "4. **Previsões** usando machine learning\n",
    "5. **Dashboards executivos** com KPIs\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Próximos Passos**\n",
    "\n",
    "### **Melhorias Planejadas:**\n",
    "1. 📊 **Dashboards interativos** com múltiplas séries\n",
    "2. 🔗 **Matriz de correlação** visual\n",
    "3. 📈 **Análise de tendências** automática\n",
    "4. 🚨 **Sistema de alertas** para mudanças significativas\n",
    "5. 🤖 **ML models** para previsões\n",
    "6. 📱 **Interface web** para usuários finais\n",
    "\n",
    "---\n",
    "\n",
    "## 🇧🇷 **Impacto para Análise Financeira Brasileira**\n",
    "\n",
    "Este sistema fornece uma **plataforma unificada** para análise de dados financeiros brasileiros, combinando:\n",
    "\n",
    "- ✅ **Dados oficiais** (BACEN, IBGE, IPEA)\n",
    "- ✅ **Dados de mercado** (B3, Yahoo Finance)  \n",
    "- ✅ **Processamento moderno** (Parquet, Lakehouse)\n",
    "- ✅ **Visualização interativa** (Plotly)\n",
    "- ✅ **Escalabilidade** (MinIO, arquitetura em camadas)\n",
    "\n",
    "**Resultado**: Capacidade de análise temporal completa da economia brasileira desde 1944 até 2025, com **mais de 800 mil registros** processados de forma otimizada e visualizados interativamente.\n",
    "\n",
    "---\n",
    "\n",
    "*🎉 **Sistema completo de lakehouse financeiro brasileiro implementado com sucesso!***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-finance-lakehouse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
